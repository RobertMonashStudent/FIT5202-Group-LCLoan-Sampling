{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n",
       "import spark.implicits._\n",
       "import org.apache.spark.ml.feature.StringIndexer\n",
       "import org.apache.spark.ml.feature.OneHotEncoderEstimator\n",
       "import org.apache.spark.ml.feature.VectorAssembler\n",
       "import org.apache.spark.ml.feature.StandardScaler\n",
       "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
       "import org.apache.spark.ml.classification.LogisticRegression\n",
       "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
       "import org.apache.spark.mllib.linalg.{Vector, Vectors}\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import spark.implicits._\n",
    "\n",
    "import org.apache.spark.ml.feature.StringIndexer\n",
    "import org.apache.spark.ml.feature.OneHotEncoderEstimator\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.ml.feature.StandardScaler\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
    "import org.apache.spark.mllib.linalg.{Vector, Vectors}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-01 10:56:05,035 WARN  [Thread-4] util.Utils (Logging.scala:logWarning(66)) - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.\n",
      "(20070,86)"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [loan_amnt: int, funded_amnt: int ... 84 more fields]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read.format(\"csv\")\n",
    "  .option(\"inferSchema\", \"true\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .load(\"../Loan_2017_20k.csv\")\n",
    "\n",
    "print((df.count(), df.columns.length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "categorical_features: Array[String] = Array(verification_status, loan_status, initial_list_status, home_ownership, acc_now_delinq)\n",
       "encodedFeatures: Array[org.apache.spark.ml.Estimator[_ >: org.apache.spark.ml.feature.OneHotEncoderModel with org.apache.spark.ml.feature.StringIndexerModel <: org.apache.spark.ml.Model[_ >: org.apache.spark.ml.feature.OneHotEncoderModel with org.apache.spark.ml.feature.StringIndexerModel <: org.apache.spark.ml.Transformer with org.apache.spark.ml.param.shared.HasHandleInvalid with org.apache.spark.ml.util.MLWritable] with org.apache.spark.ml.param.shared.HasHandleInvalid with org.apache.spark.ml.util.MLWritable] with org.apache.spark.ml.param.shared.HasHandleInvalid with org.apache.spark.ml.util.DefaultParamsWritable{def copy(extra: org.apache.spark.ml.para...\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// https://towardsdatascience.com/feature-encoding-with-spark-2-3-0-part-1-9ede45562740\n",
    "\n",
    "//val categorical_features = df.columns.filter(_.contains(\"status\"))\n",
    "\n",
    "val categorical_features = Array(\n",
    "    \"verification_status\", \n",
    "    \"loan_status\", \n",
    "    \"initial_list_status\",\n",
    "    \"home_ownership\",\n",
    "    \"acc_now_delinq\",\n",
    ")\n",
    "\n",
    "val encodedFeatures = categorical_features.flatMap{ name =>\n",
    "    \n",
    "    val stringIndexer = new StringIndexer()\n",
    "      .setInputCol(name)\n",
    "      .setOutputCol(name + \"_index\")\n",
    "    \n",
    "    val oneHotEncoder = new OneHotEncoderEstimator()\n",
    "      .setInputCols(Array(name + \"_index\"))\n",
    "      .setOutputCols(Array(name + \"_vec\"))\n",
    "      .setDropLast(false)\n",
    "    \n",
    "    Array(stringIndexer, oneHotEncoder)\n",
    "}\n",
    "\n",
    "val pipeline = new Pipeline()\n",
    "  .setStages(encodedFeatures)\n",
    "\n",
    "val df_transformed = pipeline\n",
    "  .fit(df)\n",
    "  .transform(df)\n",
    "\n",
    "//df_transformed.columns.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verification_status\n",
      "loan_status\n",
      "initial_list_status\n"
     ]
    }
   ],
   "source": [
    "df.columns\n",
    "  .filter(_.contains(\"status\"))\n",
    "  .toArray\n",
    "  .foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verification_status\n",
      "loan_status\n",
      "initial_list_status\n",
      "verification_status_index\n",
      "verification_status_vec\n",
      "loan_status_index\n",
      "loan_status_vec\n",
      "initial_list_status_index\n",
      "initial_list_status_vec\n"
     ]
    }
   ],
   "source": [
    "df_transformed.columns\n",
    "  .filter(_.contains(\"status\"))\n",
    "  .toArray\n",
    "  .foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n",
      "|       loan_status|count|\n",
      "+------------------+-----+\n",
      "|           Default|    1|\n",
      "| Late (16-30 days)|   58|\n",
      "|   In Grace Period|  136|\n",
      "|Late (31-120 days)|  367|\n",
      "|       Charged Off| 1660|\n",
      "|        Fully Paid| 5446|\n",
      "|           Current|12402|\n",
      "+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_transformed\n",
    "  .groupBy(\"loan_status\")\n",
    "  .count()\n",
    "  .sort(\"count\")\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|loan_status_index|count|\n",
      "+-----------------+-----+\n",
      "|              6.0|    1|\n",
      "|              5.0|   58|\n",
      "|              4.0|  136|\n",
      "|              3.0|  367|\n",
      "|              2.0| 1660|\n",
      "|              1.0| 5446|\n",
      "|              0.0|12402|\n",
      "+-----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_transformed\n",
    "  .groupBy(\"loan_status_index\")\n",
    "  .count()\n",
    "  .sort(\"count\")\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|loan_status_vec|count|\n",
      "+---------------+-----+\n",
      "|  (7,[6],[1.0])|    1|\n",
      "|  (7,[5],[1.0])|   58|\n",
      "|  (7,[4],[1.0])|  136|\n",
      "|  (7,[3],[1.0])|  367|\n",
      "|  (7,[2],[1.0])| 1660|\n",
      "|  (7,[1],[1.0])| 5446|\n",
      "|  (7,[0],[1.0])|12402|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_transformed.groupBy(\"loan_status_vec\")\n",
    "  .count()\n",
    "  .sort(\"count\")\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vecFeatures: Array[String] = Array(verification_status_vec, loan_status_vec, initial_list_status_vec, home_ownership_vec, acc_now_delinq_vec)\n",
       "vectorAssembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_19e24e6a34bb\n",
       "pipelineVectorAssembler: org.apache.spark.ml.Pipeline = pipeline_1fe7f0bcecd3\n",
       "result_df: org.apache.spark.sql.DataFrame = [loan_amnt: int, funded_amnt: int ... 95 more fields]\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val vecFeatures = df_transformed\n",
    "  .columns.filter(_.contains(\"vec\")).toArray\n",
    "\n",
    "val vectorAssembler = new VectorAssembler()\n",
    "  .setInputCols(vecFeatures)\n",
    "  .setOutputCol(\"categorical_features\")\n",
    "\n",
    "val pipelineVectorAssembler = new Pipeline()\n",
    "  .setStages(Array(vectorAssembler))\n",
    "\n",
    "val result_df = pipelineVectorAssembler\n",
    "  .fit(df_transformed)\n",
    "  .transform(df_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+\n",
      "|numerical_features|categorical_features|\n",
      "+------------------+--------------------+\n",
      "| [82000.0,14000.0]|(19,[0,4,10,12,16...|\n",
      "| [69000.0,10000.0]|(19,[1,3,11,13,16...|\n",
      "| [215000.0,5000.0]|(19,[0,3,11,12,16...|\n",
      "|  [50000.0,4000.0]|(19,[0,5,10,13,16...|\n",
      "| [89000.0,14000.0]|(19,[1,4,10,12,16...|\n",
      "|[110000.0,15000.0]|(19,[0,5,10,12,16...|\n",
      "| [70000.0,15750.0]|(19,[1,4,10,12,16...|\n",
      "|  [73000.0,7800.0]|(19,[1,4,10,12,16...|\n",
      "|[130000.0,28000.0]|(19,[0,3,11,13,16...|\n",
      "| [38000.0,10000.0]|(19,[2,6,10,13,16...|\n",
      "| [81000.0,16000.0]|(19,[0,5,10,12,16...|\n",
      "|  [20832.0,3000.0]|(19,[2,3,11,13,16...|\n",
      "|  [60000.0,4000.0]|(19,[0,3,10,13,16...|\n",
      "| [76000.0,24000.0]|(19,[1,3,10,12,16...|\n",
      "| [47000.0,12000.0]|(19,[1,3,10,12,16...|\n",
      "| [98440.0,13000.0]|(19,[1,4,11,13,16...|\n",
      "| [75000.0,21000.0]|(19,[1,3,11,12,16...|\n",
      "| [70000.0,30800.0]|(19,[2,3,10,14,16...|\n",
      "| [83000.0,30000.0]|(19,[0,3,10,12,16...|\n",
      "|  [40000.0,7000.0]|(19,[1,3,10,13,16...|\n",
      "+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numerical_features: Array[String] = Array(annual_inc, loan_amnt)\n",
       "vectorAssembler2: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_3c9034193868\n",
       "pipelineVectorAssembler2: org.apache.spark.ml.Pipeline = pipeline_e9df290bec63\n",
       "result_df: org.apache.spark.sql.DataFrame = [loan_amnt: int, funded_amnt: int ... 96 more fields]\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//https://towardsdatascience.com/feature-encoding-made-simple-with-spark-2-3-0-part-2-5bfc869a809a\n",
    "\n",
    "val numerical_features = Array(\"annual_inc\", \"loan_amnt\")\n",
    "\n",
    "val vectorAssembler2 = new VectorAssembler()\n",
    "  .setInputCols(numerical_features)\n",
    "  .setOutputCol(\"numerical_features\")\n",
    "\n",
    "val pipelineVectorAssembler2 = new Pipeline()\n",
    "  .setStages(Array(vectorAssembler, vectorAssembler2))\n",
    "\n",
    "val result_df = pipelineVectorAssembler2\n",
    "  .fit(df_transformed)\n",
    "  .transform(df_transformed)\n",
    "\n",
    "result_df.select(\"numerical_features\", \"categorical_features\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.sql.AnalysisException",
     "evalue": " cannot resolve 'UDF(numerical_features)' due to data type mismatch: argument 1 requires vector type, however, '`numerical_features`' is of struct<type:tinyint,size:int,indices:array<int>,values:array<double>> type.;;",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.sql.AnalysisException: cannot resolve 'UDF(numerical_features)' due to data type mismatch: argument 1 requires vector type, however, '`numerical_features`' is of struct<type:tinyint,size:int,indices:array<int>,values:array<double>> type.;;",
      "'Project [loan_amnt#10, funded_amnt#11, funded_amnt_inv#12, term#13, int_rate#14, installment#15, grade#16, sub_grade#17, emp_title#18, emp_length#19, home_ownership#20, annual_inc#21, verification_status#22, issue_d#23, loan_status#24, pymnt_plan#25, purpose#26, title#27, zip_code#28, addr_state#29, dti#30, delinq_2yrs#31, earliest_cr_line#32, inq_last_6mths#33, ... 74 more fields]",
      "+- Project [loan_amnt#10, funded_amnt#11, funded_amnt_inv#12, term#13, int_rate#14, installment#15, grade#16, sub_grade#17, emp_title#18, emp_length#19, home_ownership#20, annual_inc#21, verification_status#22, issue_d#23, loan_status#24, pymnt_plan#25, purpose#26, title#27, zip_code#28, addr_state#29, dti#30, delinq_2yrs#31, earliest_cr_line#32, inq_last_6mths#33, ... 74 more fields]",
      "   +- Project [loan_amnt#10, funded_amnt#11, funded_amnt_inv#12, term#13, int_rate#14, installment#15, grade#16, sub_grade#17, emp_title#18, emp_length#19, home_ownership#20, annual_inc#21, verification_status#22, issue_d#23, loan_status#24, pymnt_plan#25, purpose#26, title#27, zip_code#28, addr_state#29, dti#30, delinq_2yrs#31, earliest_cr_line#32, inq_last_6mths#33, ... 73 more fields]",
      "      +- Project [loan_amnt#10, funded_amnt#11, funded_amnt_inv#12, term#13, int_rate#14, installment#15, grade#16, sub_grade#17, emp_title#18, emp_length#19, home_ownership#20, annual_inc#21, verification_status#22, issue_d#23, loan_status#24, pymnt_plan#25, purpose#26, title#27, zip_code#28, addr_state#29, dti#30, delinq_2yrs#31, earliest_cr_line#32, inq_last_6mths#33, ... 72 more fields]",
      "         +- Project [loan_amnt#10, funded_amnt#11, funded_amnt_inv#12, term#13, int_rate#14, installment#15, grade#16, sub_grade#17, emp_title#18, emp_length#19, home_ownership#20, annual_inc#21, verification_status#22, issue_d#23, loan_status#24, pymnt_plan#25, purpose#26, title#27, zip_code#28, addr_state#29, dti#30, delinq_2yrs#31, earliest_cr_line#32, inq_last_6mths#33, ... 71 more fields]",
      "            +- Project [loan_amnt#10, funded_amnt#11, funded_amnt_inv#12, term#13, int_rate#14, installment#15, grade#16, sub_grade#17, emp_title#18, emp_length#19, home_ownership#20, annual_inc#21, verification_status#22, issue_d#23, loan_status#24, pymnt_plan#25, purpose#26, title#27, zip_code#28, addr_state#29, dti#30, delinq_2yrs#31, earliest_cr_line#32, inq_last_6mths#33, ... 70 more fields]",
      "               +- Project [loan_amnt#10, funded_amnt#11, funded_amnt_inv#12, term#13, int_rate#14, installment#15, grade#16, sub_grade#17, emp_title#18, emp_length#19, home_ownership#20, annual_inc#21, verification_status#22, issue_d#23, loan_status#24, pymnt_plan#25, purpose#26, title#27, zip_code#28, addr_state#29, dti#30, delinq_2yrs#31, earliest_cr_line#32, inq_last_6mths#33, ... 69 more fields]",
      "                  +- Project [loan_amnt#10, funded_amnt#11, funded_amnt_inv#12, term#13, int_rate#14, installment#15, grade#16, sub_grade#17, emp_title#18, emp_length#19, home_ownership#20, annual_inc#21, verification_status#22, issue_d#23, loan_status#24, pymnt_plan#25, purpose#26, title#27, zip_code#28, addr_state#29, dti#30, delinq_2yrs#31, earliest_cr_line#32, inq_last_6mths#33, ... 68 more fields]",
      "                     +- Project [loan_amnt#10, funded_amnt#11, funded_amnt_inv#12, term#13, int_rate#14, installment#15, grade#16, sub_grade#17, emp_title#18, emp_length#19, home_ownership#20, annual_inc#21, verification_status#22, issue_d#23, loan_status#24, pymnt_plan#25, purpose#26, title#27, zip_code#28, addr_state#29, dti#30, delinq_2yrs#31, earliest_cr_line#32, inq_last_6mths#33, ... 67 more fields]",
      "                        +- Project [loan_amnt#10, funded_amnt#11, funded_amnt_inv#12, term#13, int_rate#14, installment#15, grade#16, sub_grade#17, emp_title#18, emp_length#19, home_ownership#20, annual_inc#21, verification_status#22, issue_d#23, loan_status#24, pymnt_plan#25, purpose#26, title#27, zip_code#28, addr_state#29, dti#30, delinq_2yrs#31, earliest_cr_line#32, inq_last_6mths#33, ... 66 more fields]",
      "                           +- Project [loan_amnt#10, funded_amnt#11, funded_amnt_inv#12, term#13, int_rate#14, installment#15, grade#16, sub_grade#17, emp_title#18, emp_length#19, home_ownership#20, annual_inc#21, verification_status#22, issue_d#23, loan_status#24, pymnt_plan#25, purpose#26, title#27, zip_code#28, addr_state#29, dti#30, delinq_2yrs#31, earliest_cr_line#32, inq_last_6mths#33, ... 65 more fields]",
      "                              +- Project [loan_amnt#10, funded_amnt#11, funded_amnt_inv#12, term#13, int_rate#14, installment#15, grade#16, sub_grade#17, emp_title#18, emp_length#19, home_ownership#20, annual_inc#21, verification_status#22, issue_d#23, loan_status#24, pymnt_plan#25, purpose#26, title#27, zip_code#28, addr_state#29, dti#30, delinq_2yrs#31, earliest_cr_line#32, inq_last_6mths#33, ... 64 more fields]",
      "                                 +- Project [loan_amnt#10, funded_amnt#11, funded_amnt_inv#12, term#13, int_rate#14, installment#15, grade#16, sub_grade#17, emp_title#18, emp_length#19, home_ownership#20, annual_inc#21, verification_status#22, issue_d#23, loan_status#24, pymnt_plan#25, purpose#26, title#27, zip_code#28, addr_state#29, dti#30, delinq_2yrs#31, earliest_cr_line#32, inq_last_6mths#33, ... 63 more fields]",
      "                                    +- Relation[loan_amnt#10,funded_amnt#11,funded_amnt_inv#12,term#13,int_rate#14,installment#15,grade#16,sub_grade#17,emp_title#18,emp_length#19,home_ownership#20,annual_inc#21,verification_status#22,issue_d#23,loan_status#24,pymnt_plan#25,purpose#26,title#27,zip_code#28,addr_state#29,dti#30,delinq_2yrs#31,earliest_cr_line#32,inq_last_6mths#33,... 62 more fields] csv",
      "",
      "  at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(CheckAnalysis.scala:116)",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(CheckAnalysis.scala:107)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$2(TreeNode.scala:278)",
      "  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:278)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$1(TreeNode.scala:275)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:326)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:324)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:275)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUp$1(QueryPlan.scala:93)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:105)",
      "  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:105)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:116)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$3(QueryPlan.scala:121)",
      "  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:237)",
      "  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)",
      "  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)",
      "  at scala.collection.TraversableLike.map(TraversableLike.scala:237)",
      "  at scala.collection.TraversableLike.map$(TraversableLike.scala:230)",
      "  at scala.collection.AbstractTraversable.map(Traversable.scala:108)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:121)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:126)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:107)",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:85)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:85)",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:82)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:108)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)",
      "  at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)",
      "  at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)",
      "  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)",
      "  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)",
      "  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3406)",
      "  at org.apache.spark.sql.Dataset.select(Dataset.scala:1335)",
      "  at org.apache.spark.sql.Dataset.withColumns(Dataset.scala:2252)",
      "  at org.apache.spark.sql.Dataset.withColumn(Dataset.scala:2219)",
      "  ... 41 elided",
      ""
     ]
    }
   ],
   "source": [
    "//convert the sparse vector to a dense vector as a fail safe\n",
    "\n",
    "val sparseToDense = udf((v : Vector) => v.toDense)\n",
    "val result_df_dense = result_df.withColumn(\"numerical_features\", sparseToDense($\"numerical_features\"))\n",
    "\n",
    "/**\n",
    "# convert the data to dense vector\n",
    "def transData(data):\n",
    "    return data.rdd.map(lambda r: [Vectors.dense(r[:-1])]).toDF(['features'])\n",
    "**/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+--------------------+\n",
      "|numerical_features|      scaledFeatures|categorical_features|\n",
      "+------------------+--------------------+--------------------+\n",
      "| [82000.0,14000.0]|[0.01694873096501...|(19,[0,4,10,12,16...|\n",
      "| [69000.0,10000.0]|[-0.1248892191143...|(19,[1,3,11,13,16...|\n",
      "| [215000.0,5000.0]|[1.46806006639242...|(19,[0,3,11,12,16...|\n",
      "|  [50000.0,4000.0]|[-0.3321908384611...|(19,[0,5,10,13,16...|\n",
      "| [89000.0,14000.0]|[0.09332301177698...|(19,[1,4,10,12,16...|\n",
      "|[110000.0,15000.0]|[0.32244585421288...|(19,[0,5,10,12,16...|\n",
      "| [70000.0,15750.0]|[-0.1139786075697...|(19,[1,4,10,12,16...|\n",
      "|  [73000.0,7800.0]|[-0.0812467729360...|(19,[1,4,10,12,16...|\n",
      "|[130000.0,28000.0]|[0.54065808510422...|(19,[0,3,11,13,16...|\n",
      "| [38000.0,10000.0]|[-0.4631181769959...|(19,[2,6,10,13,16...|\n",
      "| [81000.0,16000.0]|[0.00603811942044...|(19,[0,5,10,12,16...|\n",
      "|  [20832.0,3000.0]|[-0.6504315559930...|(19,[2,3,11,13,16...|\n",
      "|  [60000.0,4000.0]|[-0.2230847230154...|(19,[0,3,10,13,16...|\n",
      "| [76000.0,24000.0]|[-0.0485149383023...|(19,[1,3,10,12,16...|\n",
      "| [47000.0,12000.0]|[-0.3649226730948...|(19,[1,3,10,12,16...|\n",
      "| [98440.0,13000.0]|[0.19631918475769...|(19,[1,4,11,13,16...|\n",
      "| [75000.0,21000.0]|[-0.0594255498469...|(19,[1,3,11,12,16...|\n",
      "| [70000.0,30800.0]|[-0.1139786075697...|(19,[2,3,10,14,16...|\n",
      "| [83000.0,30000.0]|[0.02785934250958...|(19,[0,3,10,12,16...|\n",
      "|  [40000.0,7000.0]|[-0.4412969539067...|(19,[1,3,10,13,16...|\n",
      "+------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "scaler: org.apache.spark.ml.feature.StandardScaler = stdScal_73180b1a09a2\n",
       "scaledData: org.apache.spark.sql.DataFrame = [loan_amnt: int, funded_amnt: int ... 97 more fields]\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val scaler = new StandardScaler()\n",
    "  .setInputCol(\"numerical_features\")\n",
    "  .setOutputCol(\"scaledFeatures\")\n",
    "  .setWithStd(true)\n",
    "  .setWithMean(true)\n",
    "\n",
    "// Normalize each feature to have unit standard deviation.\n",
    "val scaledData = scaler\n",
    "  .fit(result_df)\n",
    "  .transform(result_df)\n",
    "\n",
    "scaledData.select(\"numerical_features\", \"scaledFeatures\", \"categorical_features\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|(21,[0,4,10,12,16...|\n",
      "|(21,[1,3,11,13,16...|\n",
      "|(21,[0,3,11,12,16...|\n",
      "|(21,[0,5,10,13,16...|\n",
      "|(21,[1,4,10,12,16...|\n",
      "|(21,[0,5,10,12,16...|\n",
      "|(21,[1,4,10,12,16...|\n",
      "|(21,[1,4,10,12,16...|\n",
      "|(21,[0,3,11,13,16...|\n",
      "|(21,[2,6,10,13,16...|\n",
      "|(21,[0,5,10,12,16...|\n",
      "|(21,[2,3,11,13,16...|\n",
      "|(21,[0,3,10,13,16...|\n",
      "|(21,[1,3,10,12,16...|\n",
      "|(21,[1,3,10,12,16...|\n",
      "|(21,[1,4,11,13,16...|\n",
      "|(21,[1,3,11,12,16...|\n",
      "|(21,[2,3,10,14,16...|\n",
      "|(21,[0,3,10,12,16...|\n",
      "|(21,[1,3,10,13,16...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "vectorAssembler3: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_cbb67173b158\n",
       "pipelineVectorAssembler3: org.apache.spark.ml.Pipeline = pipeline_d6ff0194bb0f\n",
       "result_df: org.apache.spark.sql.DataFrame = [loan_amnt: int, funded_amnt: int ... 98 more fields]\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val vectorAssembler3 = new VectorAssembler()\n",
    "  .setInputCols(Array(\"categorical_features\", \"scaledFeatures\"))\n",
    "  .setOutputCol(\"features\")\n",
    "\n",
    "val pipelineVectorAssembler3 = new Pipeline()\n",
    "  .setStages(Array(vectorAssembler3))\n",
    "\n",
    "val result_df = pipelineVectorAssembler3\n",
    "  .fit(scaledData)\n",
    "  .transform(scaledData)\n",
    "\n",
    "result_df.select(\"features\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labelIndexer: org.apache.spark.ml.feature.StringIndexer = strIdx_d96d87ff5786\n",
       "df3: org.apache.spark.sql.DataFrame = [loan_amnt: int, funded_amnt: int ... 99 more fields]\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// create the labels column\n",
    "val labelIndexer = new StringIndexer()\n",
    "  .setInputCol(\"term\")\n",
    "  .setOutputCol(\"label\")\n",
    "\n",
    "val df3 = labelIndexer\n",
    "  .fit(result_df)\n",
    "  .transform(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model: org.apache.spark.ml.classification.LogisticRegressionModel = LogisticRegressionModel: uid = logreg_b34c6c1f9a72, numClasses = 2, numFeatures = 21\n",
       "predictions: org.apache.spark.sql.DataFrame = [loan_amnt: int, funded_amnt: int ... 102 more fields]\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// https://www.bmc.com/blogs/using-logistic-regression-scala-spark/\n",
    "\n",
    "val model = new LogisticRegression().fit(df3)\n",
    "\n",
    "val predictions = model.transform(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+----------+\n",
      "|            features|label|prediction|\n",
      "+--------------------+-----+----------+\n",
      "|(21,[0,4,10,12,16...|  0.0|       0.0|\n",
      "|(21,[1,3,11,13,16...|  0.0|       0.0|\n",
      "|(21,[0,3,11,12,16...|  0.0|       0.0|\n",
      "|(21,[0,5,10,13,16...|  0.0|       0.0|\n",
      "|(21,[1,4,10,12,16...|  0.0|       0.0|\n",
      "|(21,[0,5,10,12,16...|  1.0|       0.0|\n",
      "|(21,[1,4,10,12,16...|  1.0|       0.0|\n",
      "|(21,[1,4,10,12,16...|  0.0|       0.0|\n",
      "|(21,[0,3,11,13,16...|  0.0|       0.0|\n",
      "|(21,[2,6,10,13,16...|  1.0|       0.0|\n",
      "|(21,[0,5,10,12,16...|  1.0|       0.0|\n",
      "|(21,[2,3,11,13,16...|  0.0|       0.0|\n",
      "|(21,[0,3,10,13,16...|  0.0|       0.0|\n",
      "|(21,[1,3,10,12,16...|  1.0|       1.0|\n",
      "|(21,[1,3,10,12,16...|  1.0|       0.0|\n",
      "|(21,[1,4,11,13,16...|  0.0|       0.0|\n",
      "|(21,[1,3,11,12,16...|  0.0|       0.0|\n",
      "|(21,[2,3,10,14,16...|  1.0|       1.0|\n",
      "|(21,[0,3,10,12,16...|  1.0|       1.0|\n",
      "|(21,[1,3,10,13,16...|  0.0|       0.0|\n",
      "+--------------------+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions\n",
    "  .select (\"features\", \"label\", \"prediction\")\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "areaUnderROC = 0.6403947867776694\n",
      "areaUnderPR = 0.5044704154926782\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "binaryClassificationEvaluator: org.apache.spark.ml.evaluation.BinaryClassificationEvaluator = binEval_554a0f51f9a1\n",
       "printlnMetric: (metricName: String)Unit\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//https://stackoverflow.com/questions/37566321/spark-random-forest-binary-classifier-metrics\n",
    "\n",
    "\n",
    "val binaryClassificationEvaluator = new BinaryClassificationEvaluator()\n",
    "  .setLabelCol(\"label\")\n",
    "  .setRawPredictionCol(\"prediction\")\n",
    "\n",
    "def printlnMetric(metricName: String): Unit = {\n",
    "  println(metricName + \" = \" + binaryClassificationEvaluator\n",
    "                                 .setMetricName(metricName)\n",
    "                                 .evaluate(predictions))\n",
    "}\n",
    "\n",
    "printlnMetric(\"areaUnderROC\")\n",
    "printlnMetric(\"areaUnderPR\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
