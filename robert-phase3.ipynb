{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Transformation and Modelling Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.StringIndexer\n",
       "import org.apache.spark.ml.feature.OneHotEncoderEstimator\n",
       "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
       "import org.apache.spark.ml.feature._\n",
       "import org.apache.spark.mllib.feature.Normalizer\n",
       "import org.apache.spark.ml.feature.VectorAssembler\n",
       "import org.apache.spark.ml.classification.LogisticRegression\n",
       "import org.apache.spark.ml.classification._\n",
       "import org.apache.spark.sql._\n"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.StringIndexer\n",
    "import org.apache.spark.ml.feature.OneHotEncoderEstimator\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.feature._\n",
    "import org.apache.spark.mllib.feature.Normalizer\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.classification._\n",
    "import org.apache.spark.sql._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// pipline: dataframe, modeltype, cutoff, weighted? y/n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [loan_amnt: int, term: string ... 63 more fields]\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read.format(\"csv\")\n",
    "  .option(\"inferSchema\", \"true\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .load(\"LCLoan_Wrangled.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "categorical_features: Array[String] = Array(emp_length, term, zip_code, grade, sub_grade, title, purpose, application_type, addr_state, home_ownership, verification_status, initial_list_status)\n",
       "encodeCategoric: Array[org.apache.spark.ml.Estimator[_ >: org.apache.spark.ml.feature.OneHotEncoderModel with org.apache.spark.ml.feature.StringIndexerModel <: org.apache.spark.ml.Model[_ >: org.apache.spark.ml.feature.OneHotEncoderModel with org.apache.spark.ml.feature.StringIndexerModel <: org.apache.spark.ml.Transformer with org.apache.spark.ml.param.shared.HasHandleInvalid with org.apache.spark.ml.util.MLWritable] with org.apache.spark.ml.param.shared.HasHandleInvalid with org.apache.spark.ml.util.MLWritable] with org.apache.spark.ml.param.shared.HasHandleInvalid with org.apache.spark.ml.util..."
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Categorical Features array\n",
    "val categorical_features = Array(\n",
    "    \"emp_length\", \"term\", \"zip_code\", \"grade\", \"sub_grade\", \"title\", \"purpose\", \n",
    "    \"application_type\", \"addr_state\", \"home_ownership\",\n",
    "     \"verification_status\", \"initial_list_status\"\n",
    ")\n",
    "\n",
    "//encode every categorical feature\n",
    "val encodeCategoric = categorical_features.flatMap{ name =>\n",
    "    \n",
    "    val stringIndexer = new StringIndexer()\n",
    "      .setInputCol(name)\n",
    "      .setOutputCol(name + \"_index\")\n",
    "      .setHandleInvalid(\"skip\") // options are \"keep\", \"error\" or \"skip\"\n",
    "    \n",
    "    val oneHotEncoder = new OneHotEncoderEstimator()\n",
    "      .setInputCols(Array(name + \"_index\"))\n",
    "      .setOutputCols(Array(name + \"_vec\"))\n",
    "      .setDropLast(false)\n",
    "    \n",
    "    Array(stringIndexer, oneHotEncoder)\n",
    "}\n",
    "\n",
    "//define a list of vecotr columns (will be created by the above stage)\n",
    "val categoricVectorCols = categorical_features.map(_ + \"_vec\")\n",
    "\n",
    "//combine categoric vector columns into one\n",
    "val categoricVectorCombine = new VectorAssembler()\n",
    "  .setInputCols(categoricVectorCols)\n",
    "  .setOutputCol(\"categorical_features\")\n",
    "\n",
    "//Get the number-type columns from the Dataframe\n",
    "val numericColumns = df.dtypes.filter(column => column._2 == \"IntegerType\" || column._2 == \"DoubleType\").map(_._1)\n",
    "    \n",
    "\n",
    "//remove the target variable from the numberColumns\n",
    "numberColumns = numberColumns diff Array(\"loan_status\")\n",
    "    \n",
    "//define Vector Assembler for the numeric columns\n",
    "val numericVec = new VectorAssembler()\n",
    "    .setInputCols(numberColumns)\n",
    "    .setOutputCol(\"numerical_features\")\n",
    "    .setHandleInvalid(\"skip\") \n",
    "    \n",
    "//Numeric scaler, this one is a  minmax scaler\n",
    "val scaler = new MinMaxScaler()\n",
    "    .setInputCol(\"numerical_features\")\n",
    "    .setOutputCol(\"scaledFeatures\")\n",
    "\n",
    "//Combine Features\n",
    "val combineAllVec = new VectorAssembler()\n",
    "    .setInputCols(Array(\"categorical_features\", \"scaledFeatures\"))\n",
    "    .setOutputCol(\"features\")\n",
    "\n",
    "//set the Transformation Order\n",
    "val transformOrder = Array(numericVec,scaler) ++ encodeCategoric ++ Array(categoricVectorCombine,combineAllVec) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the Modelling Options\n",
    "The function will accept inputs to determine the appropriate Pipline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "runModel: (df: org.apache.spark.sql.DataFrame, modelType: String, threshold: Double, weighted: Boolean)org.apache.spark.ml.PipelineModel\n"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def runModel ( df : DataFrame, modelType : String, threshold : Double, weighted : Boolean ) : PipelineModel = {\n",
    "        \n",
    "    //default\n",
    "    val pipelineOrder = transformOrder    \n",
    "    \n",
    "    \n",
    "    //depending on the model chosen,\n",
    "    if (modelType == \"lr\"){\n",
    "        \n",
    "        //logistic regression option\n",
    "        val model = new LogisticRegression()\n",
    "            .setMaxIter(10)\n",
    "            .setRegParam(0.001)\n",
    "            .setThreshold(threshold)\n",
    "        \n",
    "        //****not sure about the weighted option just yet\n",
    "        \n",
    "        \n",
    "        //set the pipeline order for this option\n",
    "        val pipelineOrder = transformOrder ++ Array(model)\n",
    "    }\n",
    "    \n",
    "    else if (modelType == \"dt\"){\n",
    "        \n",
    "        /*decisiontree option - stil working on this\n",
    "        val model = new DecisionTreeClassifier()\n",
    "            .setLabelCol(\"loan_status\")\n",
    "            .setFeaturesCol(\"features\")\n",
    "        \n",
    "        //Label Converter\n",
    "        val labelConverter = new IndexToString()\n",
    "            .setInputCol(\"prediction\")\n",
    "            .setOutputCol(\"predictedLabel\")\n",
    "            .setLabels(labelIndexer.labels)\n",
    "        \n",
    "        //set the pipeline order for this option\n",
    "        val pipelineOrder = transformOrder ++ Array(model, labelConverter)*/\n",
    "        \n",
    "    }\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    //set the pipeline for the model\n",
    "    val pipeline = new Pipeline()\n",
    "        .setStages(pipelineOrder)\n",
    "    \n",
    "    //return a model fitted on the dataframe\n",
    "    return pipeline.fit(df)\n",
    "    \n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "//nothing here yet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "testmodel: org.apache.spark.ml.PipelineModel = pipeline_8522a0474fb7\n"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//running the model with the desired parameters\n",
    "var testmodel = runModel(df,\"lr\",0.5,false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------------------------------+----------+\n",
      "|label|probability                               |prediction|\n",
      "+-----+------------------------------------------+----------+\n",
      "|1    |[0.0013796581227724393,0.9986203418772276]|1.0       |\n",
      "|1    |[0.010189470424057854,0.9898105295759422] |1.0       |\n",
      "|1    |[0.18936247706655956,0.8106375229334405]  |1.0       |\n",
      "|0    |[0.9172074055718759,0.0827925944281242]   |0.0       |\n",
      "|1    |[4.260359179034007E-4,0.9995739640820965] |1.0       |\n",
      "|0    |[0.9999898327118802,1.0167288119700322E-5]|0.0       |\n",
      "|1    |[0.0017418781360447172,0.9982581218639552]|1.0       |\n",
      "|1    |[1.61385106990636E-4,0.9998386148930095]  |1.0       |\n",
      "|1    |[0.006648019148728157,0.993351980851272]  |1.0       |\n",
      "|0    |[0.025892942657991594,0.9741070573420085] |1.0       |\n",
      "|0    |[0.7029207943029315,0.29707920569706847]  |0.0       |\n",
      "|1    |[0.34132342318276887,0.6586765768172311]  |1.0       |\n",
      "|1    |[0.2000653841423633,0.7999346158576367]   |1.0       |\n",
      "|1    |[0.010584135175642303,0.9894158648243576] |1.0       |\n",
      "|1    |[0.04481601760368555,0.9551839823963145]  |1.0       |\n",
      "|1    |[0.0022412682238539382,0.9977587317761462]|1.0       |\n",
      "|1    |[0.009283428533665011,0.9907165714663351] |1.0       |\n",
      "|1    |[0.006415649968102323,0.9935843500318977] |1.0       |\n",
      "|1    |[0.009952147198351845,0.9900478528016482] |1.0       |\n",
      "|1    |[0.03368300164625553,0.9663169983537445]  |1.0       |\n",
      "+-----+------------------------------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//running the model with loan status renamed to label\n",
    "testmodel.transform(df.withColumnRenamed(\"loan_status\", \"label\"))\n",
    "    .select(\"label\",\"probability\",\"prediction\")\n",
    "    .toDF.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
