{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.StringIndexer\n",
       "import org.apache.spark.ml.feature.OneHotEncoderEstimator\n",
       "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
       "import org.apache.spark.ml.feature._\n",
       "import org.apache.spark.mllib.feature.Normalizer\n",
       "import org.apache.spark.ml.feature.VectorAssembler\n",
       "import org.apache.spark.ml.classification.LogisticRegression\n",
       "import org.apache.spark.sql._\n"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.StringIndexer\n",
    "import org.apache.spark.ml.feature.OneHotEncoderEstimator\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.feature._\n",
    "import org.apache.spark.mllib.feature.Normalizer\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.sql._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "addNumericFeatures: (df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def addNumericFeatures ( df:DataFrame ) : DataFrame = {\n",
    "    \n",
    "    //get columns that are integers or doubles\n",
    "    val integerColumns = df.dtypes.filter(column => column._2 == \"IntegerType\").map(_._1)\n",
    "    val doubleColumn = df.dtypes.filter(column => column._2 == \"DoubleType\").map(_._1)\n",
    "    \n",
    "    //add the numeric columns together into a single array\n",
    "    var numberColumns = integerColumn ++ doubleColumn\n",
    "    \n",
    "    //remove the target variable from the numberColumns\n",
    "    numberColumns = numberColumns diff Array(\"loan_status\")\n",
    "    \n",
    "    //define Vector Assembler for the numeric columns\n",
    "    val numericVectorAssembler = new VectorAssembler()\n",
    "      .setInputCols(numberColumns)\n",
    "      .setOutputCol(\"numerical_features\")\n",
    "      .setHandleInvalid(\"skip\") \n",
    "    \n",
    "    //Pipepline to turn numeric columns into\n",
    "    val pipelineNumericVectorAssembler = new Pipeline()\n",
    "      .setStages(Array(numericVectorAssembler))\n",
    "        \n",
    "    //Assemble the numeric features vector\n",
    "    val numericVector_df = pipelineNumericVectorAssembler\n",
    "      .fit(df)\n",
    "      .transform(df)\n",
    "    \n",
    "    //numeric scaler, this one is a  minmax scaler\n",
    "    val scaler = new MinMaxScaler()\n",
    "      .setInputCol(\"numerical_features\")\n",
    "      .setOutputCol(\"scaledFeatures\")\n",
    "    \n",
    "    // Normalize each feature to have unit standard deviation.\n",
    "    val scaled_df = scaler\n",
    "      .fit(numericVector_df)\n",
    "      .transform(numericVector_df)\n",
    "    \n",
    "    return scaled_df\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "testfunctionDF: org.apache.spark.sql.DataFrame = [loan_amnt: int, term: string ... 65 more fields]\n"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val testfunctionDF = addNumericFeatures(df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|  numerical_features|      scaledFeatures|\n",
      "+--------------------+--------------------+\n",
      "|(47,[0,3,5,6,8,11...|[0.33333333333333...|\n",
      "|[10000.0,0.0,1.0,...|[0.23076923076923...|\n",
      "|[5000.0,0.0,4.0,1...|[0.10256410256410...|\n",
      "|[4000.0,1.0,0.0,1...|[0.07692307692307...|\n",
      "|(47,[0,3,5,6,8,11...|[0.33333333333333...|\n",
      "|[15000.0,0.0,1.0,...|[0.35897435897435...|\n",
      "|[15750.0,0.0,1.0,...|[0.37820512820512...|\n",
      "|[7800.0,0.0,0.0,1...|[0.17435897435897...|\n",
      "|[28000.0,0.0,1.0,...|[0.69230769230769...|\n",
      "|[10000.0,1.0,0.0,...|[0.23076923076923...|\n",
      "|[16000.0,0.0,1.0,...|[0.38461538461538...|\n",
      "|(47,[0,3,5,6,8,11...|[0.05128205128205...|\n",
      "|[4000.0,1.0,1.0,6...|[0.07692307692307...|\n",
      "|[24000.0,3.0,1.0,...|[0.58974358974358...|\n",
      "|[12000.0,0.0,1.0,...|[0.28205128205128...|\n",
      "|[13000.0,0.0,3.0,...|[0.30769230769230...|\n",
      "|[21000.0,4.0,0.0,...|[0.51282051282051...|\n",
      "|[30800.0,1.0,0.0,...|[0.76410256410256...|\n",
      "|[30000.0,0.0,2.0,...|[0.74358974358974...|\n",
      "|(47,[0,3,5,6,8,11...|[0.15384615384615...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testfunctionDF.select(\"numerical_features\",\"scaledFeatures\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# code testing below here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-05 02:18:19,424 WARN  [Thread-4] util.Utils (Logging.scala:logWarning(66)) - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.\n",
      "(20070,65)"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [loan_amnt: int, term: string ... 63 more fields]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read.format(\"csv\")\n",
    "  .option(\"inferSchema\", \"true\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .load(\"LCLoan_Wrangled.csv\")\n",
    "\n",
    "print((df.count(), df.columns.length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "integerColumn: Array[String] = Array(loan_amnt, delinq_2yrs, inq_last_6mths, open_acc, pub_rec, revol_bal, total_acc, collections_12_mths_ex_med, policy_code, acc_now_delinq, tot_coll_amt, tot_cur_bal, total_rev_hi_lim, acc_open_past_24mths, delinq_amnt, mo_sin_old_rev_tl_op, mo_sin_rcnt_rev_tl_op, mo_sin_rcnt_tl, mort_acc, mths_since_recent_bc, num_accts_ever_120_pd, num_actv_rev_tl, num_bc_sats, num_il_tl, num_rev_accts, num_sats, num_tl_120dpd_2m, num_tl_90g_dpd_24m, num_tl_op_past_12m, pub_rec_bankruptcies, tax_liens, tot_hi_cred_lim, total_il_high_credit_limit, loan_status)\n",
       "doubleColumn: Array[String] = Array(int_rate, annual_inc, dti, revol_util, out_prncp, total_pymnt, total_rec_int, total_rec_late_fee, collection_recovery_fee, last_pymnt_amnt, chargeoff_within_12_mths, mo_sin_ol..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//get columns that are integer and double type\n",
    "val integerColumn = df.dtypes.filter(column => column._2 == \"IntegerType\").map(_._1)\n",
    "val doubleColumn = df.dtypes.filter(column => column._2 == \"DoubleType\").map(_._1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numberColumn: Array[String] = Array(loan_amnt, delinq_2yrs, inq_last_6mths, open_acc, pub_rec, revol_bal, total_acc, collections_12_mths_ex_med, policy_code, acc_now_delinq, tot_coll_amt, tot_cur_bal, total_rev_hi_lim, acc_open_past_24mths, delinq_amnt, mo_sin_old_rev_tl_op, mo_sin_rcnt_rev_tl_op, mo_sin_rcnt_tl, mort_acc, mths_since_recent_bc, num_accts_ever_120_pd, num_actv_rev_tl, num_bc_sats, num_il_tl, num_rev_accts, num_sats, num_tl_120dpd_2m, num_tl_90g_dpd_24m, num_tl_op_past_12m, pub_rec_bankruptcies, tax_liens, tot_hi_cred_lim, total_il_high_credit_limit, loan_status, int_rate, annual_inc, dti, revol_util, out_prncp, total_pymnt, total_rec_int, total_rec_late_fee, collection_recovery_fee, last_pymnt_amnt, chargeoff_within_12_mths, mo_sin_old_il_acct, pct_tl_nvr_dlq, percent_bc..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//add the number columns together into single array\n",
    "var numberColumn = integerColumn ++ doubleColumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "removeColumns: Array[String] = Array(loan_status, total_il_high_credit_limit)\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//remove target variable, any other columns here may need to be removed in an updated phase 2 notebook.\n",
    "var removeColumns = Array(\"loan_status\", \"total_il_high_credit_limit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numberColumn: Array[String] = [Ljava.lang.String;@5eb9d2c0\n"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//remove the target variable\n",
    "numberColumn = numberColumn diff Array(\"loan_status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res8: Array[String] = Array(loan_amnt, delinq_2yrs, inq_last_6mths, open_acc, pub_rec, revol_bal, total_acc, collections_12_mths_ex_med, policy_code, acc_now_delinq, tot_coll_amt, tot_cur_bal, total_rev_hi_lim, acc_open_past_24mths, delinq_amnt, mo_sin_old_rev_tl_op, mo_sin_rcnt_rev_tl_op, mo_sin_rcnt_tl, mort_acc, mths_since_recent_bc, num_accts_ever_120_pd, num_actv_rev_tl, num_bc_sats, num_il_tl, num_rev_accts, num_sats, num_tl_120dpd_2m, num_tl_90g_dpd_24m, num_tl_op_past_12m, pub_rec_bankruptcies, tax_liens, tot_hi_cred_lim, int_rate, annual_inc, dti, revol_util, out_prncp, total_pymnt, total_rec_int, total_rec_late_fee, collection_recovery_fee, last_pymnt_amnt, chargeoff_within_12_mths, mo_sin_old_il_acct, pct_tl_nvr_dlq, percent_bc_gt_75)\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// check columns\n",
    "numberColumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numericVectorAssembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_8228bf2542e7\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Vector Assembler\n",
    "val numericVectorAssembler = new VectorAssembler()\n",
    "  .setInputCols(numberColumn)\n",
    "  .setOutputCol(\"numerical_features\")\n",
    "  .setHandleInvalid(\"skip\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pipelineNumericVectorAssembler: org.apache.spark.ml.Pipeline = pipeline_27d6f1d1c75f\n",
       "numericVector_df: org.apache.spark.sql.DataFrame = [loan_amnt: int, term: string ... 64 more fields]\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pipelineNumericVectorAssembler = new Pipeline()\n",
    "  .setStages(Array(numericVectorAssembler))\n",
    "\n",
    "val numericVector_df = pipelineNumericVectorAssembler\n",
    "  .fit(df)\n",
    "  .transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|  numerical_features|\n",
      "+--------------------+\n",
      "|(46,[0,3,5,6,8,11...|\n",
      "|[10000.0,0.0,1.0,...|\n",
      "|[5000.0,0.0,4.0,1...|\n",
      "|[4000.0,1.0,0.0,1...|\n",
      "|(46,[0,3,5,6,8,11...|\n",
      "|[15000.0,0.0,1.0,...|\n",
      "|[15750.0,0.0,1.0,...|\n",
      "|[7800.0,0.0,0.0,1...|\n",
      "|[28000.0,0.0,1.0,...|\n",
      "|[10000.0,1.0,0.0,...|\n",
      "|[16000.0,0.0,1.0,...|\n",
      "|(46,[0,3,5,6,8,11...|\n",
      "|[4000.0,1.0,1.0,6...|\n",
      "|[24000.0,3.0,1.0,...|\n",
      "|[12000.0,0.0,1.0,...|\n",
      "|[13000.0,0.0,3.0,...|\n",
      "|[21000.0,4.0,0.0,...|\n",
      "|[30800.0,1.0,0.0,...|\n",
      "|[30000.0,0.0,2.0,...|\n",
      "|(46,[0,3,5,6,8,11...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "numericVector_df.select(\"numerical_features\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scaler: org.apache.spark.ml.feature.MinMaxScaler = minMaxScal_c8bfe8fbf891\n"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//numeric scaler\n",
    "val scaler = new MinMaxScaler()\n",
    "  .setInputCol(\"numerical_features\")\n",
    "  .setOutputCol(\"scaledFeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scaledData: org.apache.spark.sql.DataFrame = [loan_amnt: int, term: string ... 65 more fields]\n"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "// Normalize each feature to have unit standard deviation.\n",
    "val scaledData = scaler\n",
    "  .fit(numericVector_df)\n",
    "  .transform(numericVector_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|  numerical_features|      scaledFeatures|\n",
      "+--------------------+--------------------+\n",
      "|(46,[0,3,5,6,8,11...|[0.33333333333333...|\n",
      "|[10000.0,0.0,1.0,...|[0.23076923076923...|\n",
      "|[5000.0,0.0,4.0,1...|[0.10256410256410...|\n",
      "|[4000.0,1.0,0.0,1...|[0.07692307692307...|\n",
      "|(46,[0,3,5,6,8,11...|[0.33333333333333...|\n",
      "|[15000.0,0.0,1.0,...|[0.35897435897435...|\n",
      "|[15750.0,0.0,1.0,...|[0.37820512820512...|\n",
      "|[7800.0,0.0,0.0,1...|[0.17435897435897...|\n",
      "|[28000.0,0.0,1.0,...|[0.69230769230769...|\n",
      "|[10000.0,1.0,0.0,...|[0.23076923076923...|\n",
      "|[16000.0,0.0,1.0,...|[0.38461538461538...|\n",
      "|(46,[0,3,5,6,8,11...|[0.05128205128205...|\n",
      "|[4000.0,1.0,1.0,6...|[0.07692307692307...|\n",
      "|[24000.0,3.0,1.0,...|[0.58974358974358...|\n",
      "|[12000.0,0.0,1.0,...|[0.28205128205128...|\n",
      "|[13000.0,0.0,3.0,...|[0.30769230769230...|\n",
      "|[21000.0,4.0,0.0,...|[0.51282051282051...|\n",
      "|[30800.0,1.0,0.0,...|[0.76410256410256...|\n",
      "|[30000.0,0.0,2.0,...|[0.74358974358974...|\n",
      "|(46,[0,3,5,6,8,11...|[0.15384615384615...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaledData.select(\"numerical_features\",\"scaledFeatures\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
