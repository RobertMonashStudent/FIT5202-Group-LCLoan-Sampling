{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Content\n",
    "* [Read-CSV](#Read-CSV)\n",
    "* [Categorical-variables-handling](#Categorical-variables-handling)\n",
    "* [Numeric-variables-handing](#Numeric-variables-handing)\n",
    "* [Pipeline-to-process-all-variables](#Pipeline-to-process-all-variables)\n",
    "* [Create-features-and-label](#Create-features-and-label)\n",
    "* [Split-training-set-and-test-set](#Split-training-set-and-test-set)\n",
    "* [Logistic-Regression-fitting](#Logistic-Regression-fitting)\n",
    "* [Hyper-parameter-tunning-for-logistic-regression](#Hyper-parameter-tunning-for-logistic-regression)\n",
    "* [Decision-Tree-fitting](#Decision-Tree-fitting)\n",
    "* [Random-Forest-fitting](#Random-Forest-fitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://994182a2b062:4040\n",
       "SparkContext available as 'sc' (version = 2.4.3, master = local[*], app id = local-1559706677931)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.StringIndexer\n",
       "import org.apache.spark.ml.feature.OneHotEncoderEstimator\n",
       "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
       "import org.apache.spark.ml.feature.StandardScaler\n",
       "import org.apache.spark.mllib.feature.Normalizer\n",
       "import org.apache.spark.ml.feature.VectorAssembler\n",
       "import org.apache.spark.ml.classification.LogisticRegression\n",
       "import org.apache.spark.sql._\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.StringIndexer\n",
    "import org.apache.spark.ml.feature.OneHotEncoderEstimator\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.feature.StandardScaler\n",
    "import org.apache.spark.mllib.feature.Normalizer\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.sql._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read CSV\n",
    "\n",
    "Also invert the label\n",
    "* 1: bad loan\n",
    "* 0: good loan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "invertLabel: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,IntegerType,Some(List(IntegerType)))\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val invertLabel = udf((v: Int) => v match {\n",
    "    case 0 => 1\n",
    "    case 1 => 0\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [loan_amnt: int, term: string ... 63 more fields]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read.format(\"csv\")\n",
    "  .option(\"inferSchema\", \"true\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .load(\"LCLoan_Wrangled.csv\")\n",
    "  .withColumn(\"loan_status\",invertLabel($\"loan_status\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical variables handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "categoricalColumns: Array[String] = Array(term, grade, sub_grade, emp_title, emp_length, home_ownership, verification_status, issue_d, purpose, title, zip_code, addr_state, earliest_cr_line, initial_list_status, last_pymnt_d, last_credit_pull_d, application_type)\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val categoricalColumns = df.dtypes.filter(column => column._2 == \"StringType\").map(_._1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "categoricalFeatures: Array[org.apache.spark.ml.Estimator[_ >: org.apache.spark.ml.feature.OneHotEncoderModel with org.apache.spark.ml.feature.StringIndexerModel <: org.apache.spark.ml.Model[_ >: org.apache.spark.ml.feature.OneHotEncoderModel with org.apache.spark.ml.feature.StringIndexerModel <: org.apache.spark.ml.Transformer with org.apache.spark.ml.param.shared.HasHandleInvalid with org.apache.spark.ml.util.MLWritable] with org.apache.spark.ml.param.shared.HasHandleInvalid with org.apache.spark.ml.util.MLWritable] with org.apache.spark.ml.param.shared.HasHandleInvalid with org.apache.spark.ml.util.DefaultParamsWritable{def copy(extra: org.apache.spark.ml.param.ParamMap): org.apache.spark.ml.Estimator[_ >: org.apache.spark.ml.feature.OneHotEncoderModel with org.apache.spark.ml.feature..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val categoricalFeatures = categoricalColumns.flatMap{ name =>\n",
    "    \n",
    "    val stringIndexer = new StringIndexer()\n",
    "      .setInputCol(name)\n",
    "      .setOutputCol(name + \"_index\")\n",
    "      .setHandleInvalid(\"keep\")\n",
    "    \n",
    "    val oneHotEncoder = new OneHotEncoderEstimator()\n",
    "      .setInputCols(Array(name + \"_index\"))\n",
    "      .setOutputCols(Array(name + \"_vec\"))\n",
    "      .setDropLast(false)\n",
    "    \n",
    "    Array(stringIndexer, oneHotEncoder)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Numeric variables handing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numericColumns: Array[String] = Array(loan_amnt, int_rate, annual_inc, dti, delinq_2yrs, inq_last_6mths, open_acc, pub_rec, revol_bal, revol_util, total_acc, out_prncp, total_pymnt, total_rec_int, total_rec_late_fee, collection_recovery_fee, last_pymnt_amnt, collections_12_mths_ex_med, policy_code, acc_now_delinq, tot_coll_amt, tot_cur_bal, total_rev_hi_lim, acc_open_past_24mths, chargeoff_within_12_mths, delinq_amnt, mo_sin_old_il_acct, mo_sin_old_rev_tl_op, mo_sin_rcnt_rev_tl_op, mo_sin_rcnt_tl, mort_acc, mths_since_recent_bc, num_accts_ever_120_pd, num_actv_rev_tl, num_bc_sats, num_il_tl, num_rev_accts, num_sats, num_tl_120dpd_2m, num_tl_90g_dpd_24m, num_tl_op_past_12m, pct_tl_nvr_dlq, percent_bc_gt_75, pub_rec_bankruptcies, tax_liens, tot_hi_cred_lim, total_il_high_credit_limit)\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val numericColumns = df.dtypes.filter(column => column._2 == \"IntegerType\" || column._2 == \"DoubleType\").map(_._1)\n",
    "                    .filterNot( c => c == \"loan_status\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numericColumnsAssembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_d37f19c654da\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val numericColumnsAssembler = new VectorAssembler()\n",
    "  .setInputCols(numericColumns)\n",
    "  .setOutputCol(\"numerical_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numericColumnsScaler: org.apache.spark.ml.feature.StandardScaler = stdScal_8f9332e793a1\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val numericColumnsScaler = new StandardScaler()\n",
    "      .setInputCol(\"numerical_features\")\n",
    "      .setOutputCol(\"numerical_features\" + \"_vec\")\n",
    "      .setWithStd(true)\n",
    "      .setWithMean(true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline to process all variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pipeline: org.apache.spark.ml.Pipeline = pipeline_9517c01dfefa\n",
       "df_transformed: org.apache.spark.sql.DataFrame = [loan_amnt: int, term: string ... 99 more fields]\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pipeline = new Pipeline()\n",
    "  .setStages(categoricalFeatures ++ Array(numericColumnsAssembler,numericColumnsScaler) )\n",
    "\n",
    "val df_transformed = pipeline\n",
    "  .fit(df)\n",
    "  .transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create features and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "featureColumns: Array[String] = Array(term_vec, grade_vec, sub_grade_vec, emp_title_vec, emp_length_vec, home_ownership_vec, verification_status_vec, issue_d_vec, purpose_vec, title_vec, zip_code_vec, addr_state_vec, earliest_cr_line_vec, initial_list_status_vec, last_pymnt_d_vec, last_credit_pull_d_vec, application_type_vec, numerical_features_vec)\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val featureColumns = df_transformed.columns.filter(_.contains(\"_vec\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-05 03:54:10,601 WARN  [Thread-4] util.Utils (Logging.scala:logWarning(66)) - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "featureColumnsAssembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_84cec32346f7\n",
       "df_input: org.apache.spark.sql.DataFrame = [loan_amnt: int, term: string ... 101 more fields]\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val featureColumnsAssembler = new VectorAssembler()\n",
    "  .setInputCols(featureColumns)\n",
    "  .setOutputCol(\"features\")\n",
    "\n",
    "val df_input = featureColumnsAssembler.transform(df_transformed).withColumn(\"label\",col(\"loan_status\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split training set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainingData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [loan_amnt: int, term: string ... 101 more fields]\n",
       "testData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [loan_amnt: int, term: string ... 101 more fields]\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(trainingData, testData) = df_input.randomSplit(Array(0.8, 0.2), seed = 1234L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: Long = 3967\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testData.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: Long = 16103\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingData.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lr: org.apache.spark.ml.classification.LogisticRegression = logreg_b535cc854108\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lr = new LogisticRegression()\n",
    "//   .setMaxIter(100)\n",
    "//   .setRegParam(0.3)\n",
    "//   .setElasticNetParam(0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-05 03:55:07,726 WARN  [Thread-4] netlib.BLAS (BLAS.java:<clinit>(61)) - Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "2019-06-05 03:55:07,726 WARN  [Thread-4] netlib.BLAS (BLAS.java:<clinit>(61)) - Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lrModel: org.apache.spark.ml.classification.LogisticRegressionModel = LogisticRegressionModel: uid = logreg_b535cc854108, numClasses = 2, numFeatures = 10856\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Fit the model\n",
    "val lrModel = lr.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "FPR: 0.0\n",
      "TPR: 1.0\n",
      "F-measure: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "trainingSummary: org.apache.spark.ml.classification.BinaryLogisticRegressionTrainingSummary = org.apache.spark.ml.classification.BinaryLogisticRegressionTrainingSummaryImpl@1119d9d6\n",
       "accuracy: Double = 1.0\n",
       "falsePositiveRate: Double = 0.0\n",
       "truePositiveRate: Double = 1.0\n",
       "fMeasure: Double = 1.0\n",
       "precision: Double = 1.0\n",
       "recall: Double = 1.0\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Extract the summary from the returned LogisticRegressionModel instance trained in the earlier\n",
    "// example\n",
    "val trainingSummary = lrModel.binarySummary\n",
    "val accuracy = trainingSummary.accuracy\n",
    "val falsePositiveRate = trainingSummary.weightedFalsePositiveRate\n",
    "val truePositiveRate = trainingSummary.weightedTruePositiveRate\n",
    "val fMeasure = trainingSummary.weightedFMeasure\n",
    "val precision = trainingSummary.weightedPrecision\n",
    "val recall = trainingSummary.weightedRecall\n",
    "println(s\"Accuracy: $accuracy\\nFPR: $falsePositiveRate\\nTPR: $truePositiveRate\\n\" +\n",
    "  s\"F-measure: $fMeasure\\nPrecision: $precision\\nRecall: $recall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|                 FPR|               TPR|\n",
      "+--------------------+------------------+\n",
      "|                 0.0|               0.0|\n",
      "|                 0.0|0.9270205066344994|\n",
      "|0.001799930771893...|               1.0|\n",
      "|0.011976462443752164|               1.0|\n",
      "| 0.02215299411561094|               1.0|\n",
      "| 0.03232952578746971|               1.0|\n",
      "|0.042506057459328485|               1.0|\n",
      "|0.052682589131187264|               1.0|\n",
      "| 0.06285912080304604|               1.0|\n",
      "| 0.07303565247490482|               1.0|\n",
      "| 0.08321218414676358|               1.0|\n",
      "| 0.09338871581862236|               1.0|\n",
      "| 0.10356524749048114|               1.0|\n",
      "|  0.1137417791623399|               1.0|\n",
      "| 0.12391831083419869|               1.0|\n",
      "| 0.13409484250605747|               1.0|\n",
      "| 0.14427137417791625|               1.0|\n",
      "|   0.154447905849775|               1.0|\n",
      "| 0.16462443752163378|               1.0|\n",
      "| 0.17480096919349256|               1.0|\n",
      "+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "areaUnderROC: 0.9999343209820871\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "roc: org.apache.spark.sql.DataFrame = [FPR: double, TPR: double]\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val roc = trainingSummary.roc\n",
    "roc.show()\n",
    "println(s\"areaUnderROC: ${trainingSummary.areaUnderROC}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc.createOrReplaceTempView(\"roc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/74/e50234bc82c553fecdbd566d8650801e3fe2d6d8c8d940638e3d8a7c5522/pandas-0.24.2-cp36-cp36m-manylinux1_x86_64.whl (10.1MB)\n",
      "\u001b[K     |################################| 10.1MB 30.4MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.0)\n",
      "Collecting numpy>=1.12.0 (from pandas)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/2d/e4656149cbadd3a8a0369fcd1a9c7d61cc7b87b3903b85389c70c989a696/numpy-1.16.4-cp36-cp36m-manylinux1_x86_64.whl (17.3MB)\n",
      "\u001b[K     |############################    | 15.4MB 51.0MB/s eta 0:00:01"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed numpy-1.16.4 pandas-0.24.2 pytz-2019.1\n",
      "Collecting matplotlib\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/83/d989ee20c78117c737ab40e0318ea221f1aed4e3f5a40b4f93541b369b93/matplotlib-3.1.0-cp36-cp36m-manylinux1_x86_64.whl (13.1MB)\n",
      "\u001b[K     |################################| 13.1MB 15.7MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.8.0)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading https://files.pythonhosted.org/packages/f7/d2/e07d3ebb2bd7af696440ce7e754c59dd546ffe1bbe732c8ab68b9c834e61/cycler-0.10.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.16.4)\n",
      "Collecting kiwisolver>=1.0.1 (from matplotlib)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/a1/5742b56282449b1c0968197f63eae486eca2c35dcd334bab75ad524e0de1/kiwisolver-1.1.0-cp36-cp36m-manylinux1_x86_64.whl (90kB)\n",
      "\u001b[K     |################################| 92kB 26.4MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dd/d9/3ec19e966301a6e25769976999bd7bbe552016f0d32b577dc9d63d2e0c49/pyparsing-2.4.0-py2.py3-none-any.whl (62kB)\n",
      "\u001b[K     |################################| 71kB 24.8MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.1->matplotlib) (1.11.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from kiwisolver>=1.0.1->matplotlib) (39.0.1)\n",
      "Installing collected packages: cycler, kiwisolver, pyparsing, matplotlib\n",
      "Successfully installed cycler-0.10.0 kiwisolver-1.1.0 matplotlib-3.1.0 pyparsing-2.4.0\n"
     ]
    }
   ],
   "source": [
    "! pip install pandas\n",
    "! pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        FPR       TPR\n",
      "0  0.000000  0.000000\n",
      "1  0.000000  0.927021\n",
      "2  0.001800  1.000000\n",
      "3  0.011976  1.000000\n",
      "4  0.022153  1.000000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%python\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile\n",
    "from IPython.display import Image\n",
    "\n",
    "roc = spark.sql(\"select * from roc\")\n",
    "rocPandas = roc.toPandas()\n",
    "print(rocPandas.head())\n",
    "plt.clf()\n",
    "plt.plot(rocPandas['FPR'],rocPandas['TPR'])\n",
    "with tempfile.NamedTemporaryFile(suffix=\".png\") as fo:\n",
    "    plt.savefig(fo.name)\n",
    "    retval = Image(filename=fo.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prediction: org.apache.spark.sql.DataFrame = [loan_amnt: int, term: string ... 104 more fields]\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val prediction = lrModel.transform(testData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evaluatePrediction: (prediction: org.apache.spark.sql.DataFrame)Unit\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluatePrediction(prediction: DataFrame) = {\n",
    "    prediction.createOrReplaceTempView(\"prediction\")\n",
    "    \n",
    "    spark.sql(\"\"\"\n",
    "select label,prediction,count(1) from prediction group by label,prediction\n",
    "\"\"\").show(false)\n",
    "    \n",
    "    spark.sql(\"\"\"\n",
    "select \n",
    "(TP+TN)/(P+N) as Accuracy,\n",
    "TP/(TP+FP) as Precision,\n",
    "TP/(TP+FN) as Recall,\n",
    "2*TP/(2*TP+FP+FN) as F1\n",
    "from (\n",
    "    select \n",
    "    count(1) as total, \n",
    "    sum(case when label = 1 and prediction = 1 then 1 else 0 end) as TP,\n",
    "    sum(case when label = 0 and prediction = 0 then 1 else 0 end) as TN,\n",
    "    sum(case when label = 0 and prediction = 1 then 1 else 0 end) as FP,\n",
    "    sum(case when label = 1 and prediction = 0 then 1 else 0 end) as FN,\n",
    "    sum(case when label = 1 then 1 else 0 end) as P,\n",
    "    sum(case when label = 0 then 1 else 0 end) as N\n",
    "    from prediction\n",
    "    )\n",
    "\"\"\").show(false)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------+\n",
      "|label|prediction|count(1)|\n",
      "+-----+----------+--------+\n",
      "|1    |0.0       |128     |\n",
      "|0    |0.0       |3423    |\n",
      "|1    |1.0       |300     |\n",
      "|0    |1.0       |116     |\n",
      "+-----+----------+--------+\n",
      "\n",
      "+------------------+------------------+------------------+------------------+\n",
      "|Accuracy          |Precision         |Recall            |F1                |\n",
      "+------------------+------------------+------------------+------------------+\n",
      "|0.9384925636501135|0.7211538461538461|0.7009345794392523|0.7109004739336493|\n",
      "+------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluatePrediction(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.classification.DecisionTreeClassificationModel\n",
       "import org.apache.spark.ml.classification.DecisionTreeClassifier\n",
       "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
       "dt: org.apache.spark.ml.classification.DecisionTreeClassifier = dtc_b7967bbf6db0\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.DecisionTreeClassificationModel\n",
    "import org.apache.spark.ml.classification.DecisionTreeClassifier\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "\n",
    "val dt = new DecisionTreeClassifier()\n",
    "  .setLabelCol(\"label\")\n",
    "  .setFeaturesCol(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-05 03:56:29,337 WARN  [Executor task launch worker for task 418] memory.MemoryStore (Logging.scala:logWarning(66)) - Not enough space to cache rdd_495_1 in memory! (computed 140.4 MB so far)\n",
      "2019-06-05 03:56:29,341 WARN  [Executor task launch worker for task 418] storage.BlockManager (Logging.scala:logWarning(66)) - Persisting block rdd_495_1 to disk instead.\n",
      "2019-06-05 03:56:30,778 WARN  [Executor task launch worker for task 417] memory.MemoryStore (Logging.scala:logWarning(66)) - Not enough space to cache rdd_495_0 in memory! (computed 210.9 MB so far)\n",
      "2019-06-05 03:56:30,778 WARN  [Executor task launch worker for task 417] storage.BlockManager (Logging.scala:logWarning(66)) - Persisting block rdd_495_0 to disk instead.\n",
      "2019-06-05 03:56:31,771 WARN  [Executor task launch worker for task 418] memory.MemoryStore (Logging.scala:logWarning(66)) - Not enough space to cache rdd_495_1 in memory! (computed 210.9 MB so far)\n",
      "2019-06-05 03:56:34,408 WARN  [Executor task launch worker for task 417] memory.MemoryStore (Logging.scala:logWarning(66)) - Not enough space to cache rdd_495_0 in memory! (computed 316.5 MB so far)\n",
      "2019-06-05 03:56:35,646 WARN  [Executor task launch worker for task 422] memory.MemoryStore (Logging.scala:logWarning(66)) - Not enough space to cache rdd_495_1 in memory! (computed 140.4 MB so far)\n",
      "2019-06-05 03:56:35,926 WARN  [Executor task launch worker for task 421] memory.MemoryStore (Logging.scala:logWarning(66)) - Not enough space to cache rdd_495_0 in memory! (computed 316.5 MB so far)\n",
      "2019-06-05 03:56:37,167 WARN  [Executor task launch worker for task 425] memory.MemoryStore (Logging.scala:logWarning(66)) - Not enough space to cache rdd_495_0 in memory! (computed 140.4 MB so far)\n",
      "2019-06-05 03:56:38,437 WARN  [Executor task launch worker for task 429] memory.MemoryStore (Logging.scala:logWarning(66)) - Not enough space to cache rdd_495_0 in memory! (computed 91.3 MB so far)\n",
      "2019-06-05 03:56:39,352 WARN  [Executor task launch worker for task 433] memory.MemoryStore (Logging.scala:logWarning(66)) - Not enough space to cache rdd_495_0 in memory! (computed 91.3 MB so far)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model: org.apache.spark.ml.classification.DecisionTreeClassificationModel = DecisionTreeClassificationModel (uid=dtc_b7967bbf6db0) of depth 5 with 27 nodes\n",
       "prediction: org.apache.spark.sql.DataFrame = [loan_amnt: int, term: string ... 104 more fields]\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val model = dt.fit(trainingData)\n",
    "val prediction = model.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------+\n",
      "|label|prediction|count(1)|\n",
      "+-----+----------+--------+\n",
      "|1    |0.0       |59      |\n",
      "|0    |0.0       |3477    |\n",
      "|1    |1.0       |369     |\n",
      "|0    |1.0       |62      |\n",
      "+-----+----------+--------+\n",
      "\n",
      "+------------------+------------------+------------------+------------------+\n",
      "|Accuracy          |Precision         |Recall            |F1                |\n",
      "+------------------+------------------+------------------+------------------+\n",
      "|0.9694983614822283|0.8561484918793504|0.8621495327102804|0.8591385331781141|\n",
      "+------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluatePrediction(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.classification.{RandomForestClassificationModel, RandomForestClassifier}\n",
       "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.{RandomForestClassificationModel, RandomForestClassifier}\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rf: org.apache.spark.ml.classification.RandomForestClassifier = rfc_bd8a1872fcaf\n"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rf = new RandomForestClassifier()\n",
    "  .setLabelCol(\"label\")\n",
    "  .setFeaturesCol(\"features\")\n",
    "//   .setNumTrees(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-05 03:58:18,561 WARN  [Executor task launch worker for task 889] memory.MemoryStore (Logging.scala:logWarning(66)) - Not enough space to cache rdd_761_1 in memory! (computed 140.9 MB so far)\n",
      "2019-06-05 03:58:18,562 WARN  [Executor task launch worker for task 889] storage.BlockManager (Logging.scala:logWarning(66)) - Persisting block rdd_761_1 to disk instead.\n",
      "2019-06-05 03:58:20,649 WARN  [Executor task launch worker for task 888] memory.MemoryStore (Logging.scala:logWarning(66)) - Not enough space to cache rdd_761_0 in memory! (computed 211.6 MB so far)\n",
      "2019-06-05 03:58:20,649 WARN  [Executor task launch worker for task 888] storage.BlockManager (Logging.scala:logWarning(66)) - Persisting block rdd_761_0 to disk instead.\n",
      "2019-06-05 03:58:21,329 WARN  [Executor task launch worker for task 889] memory.MemoryStore (Logging.scala:logWarning(66)) - Not enough space to cache rdd_761_1 in memory! (computed 211.6 MB so far)\n",
      "2019-06-05 03:58:24,105 WARN  [Executor task launch worker for task 888] memory.MemoryStore (Logging.scala:logWarning(66)) - Not enough space to cache rdd_761_0 in memory! (computed 317.6 MB so far)\n",
      "2019-06-05 03:58:25,198 WARN  [Executor task launch worker for task 893] memory.MemoryStore (Logging.scala:logWarning(66)) - Not enough space to cache rdd_761_1 in memory! (computed 140.9 MB so far)\n",
      "2019-06-05 03:58:25,449 WARN  [Executor task launch worker for task 892] memory.MemoryStore (Logging.scala:logWarning(66)) - Not enough space to cache rdd_761_0 in memory! (computed 317.6 MB so far)\n",
      "2019-06-05 03:58:26,573 WARN  [Executor task launch worker for task 896] memory.MemoryStore (Logging.scala:logWarning(66)) - Not enough space to cache rdd_761_0 in memory! (computed 140.9 MB so far)\n",
      "2019-06-05 03:58:27,611 WARN  [Executor task launch worker for task 900] memory.MemoryStore (Logging.scala:logWarning(66)) - Not enough space to cache rdd_761_0 in memory! (computed 91.6 MB so far)\n",
      "2019-06-05 03:58:28,729 WARN  [Executor task launch worker for task 904] memory.MemoryStore (Logging.scala:logWarning(66)) - Not enough space to cache rdd_761_0 in memory! (computed 91.6 MB so far)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model: org.apache.spark.ml.classification.RandomForestClassificationModel = RandomForestClassificationModel (uid=rfc_bd8a1872fcaf) with 20 trees\n",
       "prediction: org.apache.spark.sql.DataFrame = [loan_amnt: int, term: string ... 104 more fields]\n"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val model = rf.fit(trainingData)\n",
    "val prediction = model.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------+\n",
      "|label|prediction|count(1)|\n",
      "+-----+----------+--------+\n",
      "|1    |0.0       |428     |\n",
      "|0    |0.0       |3539    |\n",
      "+-----+----------+--------+\n",
      "\n",
      "+------------------+---------+------+---+\n",
      "|Accuracy          |Precision|Recall|F1 |\n",
      "+------------------+---------+------+---+\n",
      "|0.8921099067305268|null     |0.0   |0.0|\n",
      "+------------------+---------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluatePrediction(prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
