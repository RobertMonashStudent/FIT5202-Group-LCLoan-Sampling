{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://0637b8c1019d:4040\n",
       "SparkContext available as 'sc' (version = 2.4.2, master = local[*], app id = local-1559694022834)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n",
       "import spark.implicits._\n",
       "import org.apache.spark.ml.feature.StringIndexer\n",
       "import org.apache.spark.ml.feature.OneHotEncoderEstimator\n",
       "import org.apache.spark.ml.feature.VectorAssembler\n",
       "import org.apache.spark.ml.feature.StandardScaler\n",
       "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
       "import org.apache.spark.ml.classification.LogisticRegression\n",
       "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
       "import org.apache.spark.mllib.linalg.{Vector, Vectors}\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import spark.implicits._\n",
    "\n",
    "import org.apache.spark.ml.feature.StringIndexer\n",
    "import org.apache.spark.ml.feature.OneHotEncoderEstimator\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.ml.feature.StandardScaler\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
    "import org.apache.spark.mllib.linalg.{Vector, Vectors}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-05 00:20:41,355 WARN  [Thread-4] util.Utils (Logging.scala:logWarning(66)) - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.\n",
      "(20070,86)"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [loan_amnt: int, funded_amnt: int ... 84 more fields]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read.format(\"csv\")\n",
    "  .option(\"inferSchema\", \"true\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .load(\"../Loan_2017_20k.csv\")\n",
    "\n",
    "print((df.count(), df.columns.length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "categorical_features: Array[String] = Array(emp_length, term, zip_code, grade, sub_grade, disbursement_method, title, purpose, application_type, addr_state, home_ownership, pymnt_plan, hardship_flag, debt_settlement_flag, verification_status, initial_list_status)\n",
       "encodedFeatures: Array[org.apache.spark.ml.Estimator[_ >: org.apache.spark.ml.feature.OneHotEncoderModel with org.apache.spark.ml.feature.StringIndexerModel <: org.apache.spark.ml.Model[_ >: org.apache.spark.ml.feature.OneHotEncoderModel with org.apache.spark.ml.feature.StringIndexerModel <: org.apache.spark.ml.Transformer with org.apache.spark.ml.param.shared.HasHandleInvalid with org.apache.spark.ml.util.MLWritable] with org.apache.spark.ml.param.shared.HasHandleInvalid with org.apache.spark.ml.util.MLWritable] with org.apach...\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// https://towardsdatascience.com/feature-encoding-with-spark-2-3-0-part-1-9ede45562740\n",
    "\n",
    "//val categorical_features = df.columns.filter(_.contains(\"status\"))\n",
    "\n",
    "/*\n",
    "val categorical_features = Array(\n",
    "    \"verification_status\", \n",
    "    \"loan_status\", \n",
    "    \"initial_list_status\",\n",
    "    \"home_ownership\",\n",
    "    \"acc_now_delinq\",\n",
    ")\n",
    "*/\n",
    "\n",
    "val categorical_features = Array(\n",
    "    \"emp_length\", \"term\", \"zip_code\", \"grade\", \"sub_grade\", \"disbursement_method\", \"title\", \"purpose\", \n",
    "    \"application_type\", \"addr_state\", \"home_ownership\", \"pymnt_plan\", \"hardship_flag\", \n",
    "    \"debt_settlement_flag\", \"verification_status\", \"initial_list_status\",\n",
    ")\n",
    "\n",
    "val encodedFeatures = categorical_features.flatMap{ name =>\n",
    "    \n",
    "    val stringIndexer = new StringIndexer()\n",
    "      .setInputCol(name)\n",
    "      .setOutputCol(name + \"_index\")\n",
    "      .setHandleInvalid(\"skip\") // options are \"keep\", \"error\" or \"skip\"\n",
    "    \n",
    "    val oneHotEncoder = new OneHotEncoderEstimator()\n",
    "      .setInputCols(Array(name + \"_index\"))\n",
    "      .setOutputCols(Array(name + \"_vec\"))\n",
    "      .setDropLast(false)\n",
    "    \n",
    "    Array(stringIndexer, oneHotEncoder)\n",
    "}\n",
    "\n",
    "val pipeline = new Pipeline()\n",
    "  .setStages(encodedFeatures)\n",
    "\n",
    "val df_transformed = pipeline\n",
    "  .fit(df)\n",
    "  .transform(df)\n",
    "\n",
    "//df_transformed.columns.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verification_status\n",
      "loan_status\n",
      "initial_list_status\n"
     ]
    }
   ],
   "source": [
    "df.columns\n",
    "  .filter(_.contains(\"status\"))\n",
    "  .toArray\n",
    "  .foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verification_status\n",
      "loan_status\n",
      "initial_list_status\n",
      "verification_status_index\n",
      "verification_status_vec\n",
      "initial_list_status_index\n",
      "initial_list_status_vec\n"
     ]
    }
   ],
   "source": [
    "df_transformed.columns\n",
    "  .filter(_.contains(\"status\"))\n",
    "  .toArray\n",
    "  .foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n",
      "|       loan_status|count|\n",
      "+------------------+-----+\n",
      "|           Default|    1|\n",
      "| Late (16-30 days)|   58|\n",
      "|   In Grace Period|  136|\n",
      "|Late (31-120 days)|  367|\n",
      "|       Charged Off| 1660|\n",
      "|        Fully Paid| 5446|\n",
      "|           Current|12402|\n",
      "+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_transformed\n",
    "  .groupBy(\"loan_status\")\n",
    "  .count()\n",
    "  .sort(\"count\")\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed\n",
    "  .groupBy(\"loan_status_index\")\n",
    "  .count()\n",
    "  .sort(\"count\")\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed.groupBy(\"loan_status_vec\")\n",
    "  .count()\n",
    "  .sort(\"count\")\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vecFeatures: Array[String] = Array(emp_length_vec, term_vec, zip_code_vec, grade_vec, sub_grade_vec, disbursement_method_vec, title_vec, purpose_vec, application_type_vec, addr_state_vec, home_ownership_vec, pymnt_plan_vec, hardship_flag_vec, debt_settlement_flag_vec, verification_status_vec, initial_list_status_vec)\n",
       "vectorAssembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_0e86be1c7220\n",
       "pipelineVectorAssembler: org.apache.spark.ml.Pipeline = pipeline_71731a42aaef\n",
       "result_df: org.apache.spark.sql.DataFrame = [loan_amnt: int, funded_amnt: int ... 117 more fields]\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val vecFeatures = df_transformed\n",
    "  .columns.filter(_.contains(\"vec\")).toArray\n",
    "\n",
    "val vectorAssembler = new VectorAssembler()\n",
    "  .setInputCols(vecFeatures)\n",
    "  .setOutputCol(\"categorical_features\")\n",
    "\n",
    "val pipelineVectorAssembler = new Pipeline()\n",
    "  .setStages(Array(vectorAssembler))\n",
    "\n",
    "val result_df = pipelineVectorAssembler\n",
    "  .fit(df_transformed)\n",
    "  .transform(df_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|  numerical_features|categorical_features|\n",
      "+--------------------+--------------------+\n",
      "|[0.0,0.0,0.0,0.0,...|(983,[0,12,222,84...|\n",
      "|[0.0,1.0,1.0,3889...|(983,[5,12,469,85...|\n",
      "|[0.0,4.0,0.0,2252...|(983,[0,12,159,85...|\n",
      "|[1.0,0.0,1.0,0.0,...|(983,[2,12,650,84...|\n",
      "|[0.0,0.0,0.0,0.0,...|(983,[0,12,118,85...|\n",
      "|[0.0,1.0,0.0,0.0,...|(983,[5,13,55,848...|\n",
      "|[0.0,1.0,0.0,0.0,...|(983,[6,13,109,84...|\n",
      "|[0.0,0.0,0.0,0.0,...|(983,[11,12,96,85...|\n",
      "|[0.0,1.0,0.0,1171...|(983,[6,12,56,849...|\n",
      "|[1.0,0.0,0.0,7127...|(983,[4,13,53,849...|\n",
      "|[0.0,1.0,0.0,0.0,...|(983,[0,13,553,85...|\n",
      "|[0.0,0.0,0.0,1282...|(983,[4,12,14,851...|\n",
      "|[1.0,1.0,0.0,1604...|(983,[6,12,17,849...|\n",
      "|[3.0,1.0,0.0,1637...|(983,[11,13,33,84...|\n",
      "|[0.0,1.0,2.0,8318...|(983,[10,13,112,8...|\n",
      "|[0.0,3.0,0.0,0.0,...|(983,[3,12,185,84...|\n",
      "|[4.0,0.0,0.0,8955...|(983,[4,12,16,849...|\n",
      "|[0.0,2.0,0.0,2124...|(983,[11,13,354,8...|\n",
      "|[0.0,0.0,0.0,2710...|(983,[3,12,69,850...|\n",
      "|[0.0,0.0,0.0,0.0,...|(983,[4,12,72,849...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numerical_features: Array[String] = Array(delinq_2yrs, inq_last_6mths, pub_rec, out_prncp, out_prncp_inv, total_rec_late_fee, recoveries, collection_recovery_fee, collections_12_mths_ex_med, acc_now_delinq, tot_coll_amt, chargeoff_within_12_mths, delinq_amnt, num_accts_ever_120_pd, num_tl_120dpd_2m, num_tl_30dpd, num_tl_90g_dpd_24m, num_tl_op_past_12m, pub_rec_bankruptcies, tax_liens, policy_code, acc_open_past_24mths, num_il_tl, mort_acc, num_actv_bc_tl, num_bc_sats, num_actv_rev_tl, num_rev_tl_bal_gt_0, mo_sin_rcnt_rev_tl_op, mo_sin_rcnt_tl, mths_since_recent_bc, num_bc_tl, num_op_rev_tl, int_rate, open_acc, num_sats, num_rev_accts, percent_bc_gt_75, total_acc, dti, revol_util, bc_util, pct_tl_nvr_dlq, mo_sin_old_il_acct, mo_sin_old_rev_tl_op, installment, total_rec_int, last_pymnt_am...\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//https://towardsdatascience.com/feature-encoding-made-simple-with-spark-2-3-0-part-2-5bfc869a809a\n",
    "\n",
    "//val numerical_features = Array(\"annual_inc\", \"loan_amnt\")\n",
    "\n",
    "val numerical_features = Array(\n",
    "    \"delinq_2yrs\", \"inq_last_6mths\", \"pub_rec\", \"out_prncp\", \"out_prncp_inv\", \"total_rec_late_fee\", \n",
    "    \"recoveries\", \"collection_recovery_fee\", \"collections_12_mths_ex_med\", \"acc_now_delinq\", \"tot_coll_amt\", \n",
    "    \"chargeoff_within_12_mths\", \"delinq_amnt\", \"num_accts_ever_120_pd\", \"num_tl_120dpd_2m\", \"num_tl_30dpd\", \n",
    "    \"num_tl_90g_dpd_24m\", \"num_tl_op_past_12m\", \"pub_rec_bankruptcies\", \"tax_liens\", \"policy_code\", \n",
    "    \"acc_open_past_24mths\", \"num_il_tl\", \"mort_acc\", \"num_actv_bc_tl\", \"num_bc_sats\", \"num_actv_rev_tl\", \n",
    "    \"num_rev_tl_bal_gt_0\", \"mo_sin_rcnt_rev_tl_op\", \"mo_sin_rcnt_tl\", \"mths_since_recent_bc\", \"num_bc_tl\", \n",
    "    \"num_op_rev_tl\", \"int_rate\", \"open_acc\", \"num_sats\", \"num_rev_accts\", \"percent_bc_gt_75\", \"total_acc\", \n",
    "    \"dti\", \"revol_util\", \"bc_util\", \"pct_tl_nvr_dlq\", \"mo_sin_old_il_acct\", \"mo_sin_old_rev_tl_op\", \n",
    "    \"installment\", \"total_rec_int\", \"last_pymnt_amnt\", \"avg_cur_bal\", \"loan_amnt\", \"funded_amnt\", \n",
    "    \"funded_amnt_inv\", \"total_rec_prncp\", \"total_pymnt\", \"total_pymnt_inv\", \"bc_open_to_buy\", \n",
    "    \"total_il_high_credit_limit\", \"revol_bal\", \"total_bc_limit\", \"annual_inc\", \"total_bal_ex_mort\", \n",
    "    \"total_rev_hi_lim\", \"tot_cur_bal\", \"tot_hi_cred_lim\",\n",
    ")\n",
    "\n",
    "val vectorAssembler2 = new VectorAssembler()\n",
    "  .setInputCols(numerical_features)\n",
    "  .setOutputCol(\"numerical_features\")\n",
    "  .setHandleInvalid(\"skip\") // options are \"keep\", \"error\" or \"skip\"\n",
    "\n",
    "val pipelineVectorAssembler2 = new Pipeline()\n",
    "  .setStages(Array(vectorAssembler, vectorAssembler2))\n",
    "\n",
    "val result_df = pipelineVectorAssembler2\n",
    "  .fit(df_transformed)\n",
    "  .transform(df_transformed)\n",
    "\n",
    "result_df.select(\"numerical_features\", \"categorical_features\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//convert the sparse vector to a dense vector as a fail safe\n",
    "\n",
    "val sparseToDense = udf((v : Vector) => v.toDense)\n",
    "val result_df_dense = result_df.withColumn(\"numerical_features\", sparseToDense($\"numerical_features\"))\n",
    "\n",
    "/**\n",
    "# convert the data to dense vector\n",
    "def transData(data):\n",
    "    return data.rdd.map(lambda r: [Vectors.dense(r[:-1])]).toDF(['features'])\n",
    "**/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|  numerical_features|      scaledFeatures|categorical_features|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|[0.0,0.0,0.0,0.0,...|[-0.3490493827899...|(983,[0,12,222,84...|\n",
      "|[0.0,1.0,1.0,3889...|[-0.3490493827899...|(983,[5,12,469,85...|\n",
      "|[0.0,4.0,0.0,2252...|[-0.3490493827899...|(983,[0,12,159,85...|\n",
      "|[1.0,0.0,1.0,0.0,...|[0.76314167361979...|(983,[2,12,650,84...|\n",
      "|[0.0,0.0,0.0,0.0,...|[-0.3490493827899...|(983,[0,12,118,85...|\n",
      "|[0.0,1.0,0.0,0.0,...|[-0.3490493827899...|(983,[5,13,55,848...|\n",
      "|[0.0,1.0,0.0,0.0,...|[-0.3490493827899...|(983,[6,13,109,84...|\n",
      "|[0.0,0.0,0.0,0.0,...|[-0.3490493827899...|(983,[11,12,96,85...|\n",
      "|[0.0,1.0,0.0,1171...|[-0.3490493827899...|(983,[6,12,56,849...|\n",
      "|[1.0,0.0,0.0,7127...|[0.76314167361979...|(983,[4,13,53,849...|\n",
      "|[0.0,1.0,0.0,0.0,...|[-0.3490493827899...|(983,[0,13,553,85...|\n",
      "|[0.0,0.0,0.0,1282...|[-0.3490493827899...|(983,[4,12,14,851...|\n",
      "|[1.0,1.0,0.0,1604...|[0.76314167361979...|(983,[6,12,17,849...|\n",
      "|[3.0,1.0,0.0,1637...|[2.98752378643920...|(983,[11,13,33,84...|\n",
      "|[0.0,1.0,2.0,8318...|[-0.3490493827899...|(983,[10,13,112,8...|\n",
      "|[0.0,3.0,0.0,0.0,...|[-0.3490493827899...|(983,[3,12,185,84...|\n",
      "|[4.0,0.0,0.0,8955...|[4.09971484284891...|(983,[4,12,16,849...|\n",
      "|[0.0,2.0,0.0,2124...|[-0.3490493827899...|(983,[11,13,354,8...|\n",
      "|[0.0,0.0,0.0,2710...|[-0.3490493827899...|(983,[3,12,69,850...|\n",
      "|[0.0,0.0,0.0,0.0,...|[-0.3490493827899...|(983,[4,12,72,849...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "scaler: org.apache.spark.ml.feature.StandardScaler = stdScal_e62f074c2482\n",
       "scaledData: org.apache.spark.sql.DataFrame = [loan_amnt: int, funded_amnt: int ... 119 more fields]\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val scaler = new StandardScaler()\n",
    "  .setInputCol(\"numerical_features\")\n",
    "  .setOutputCol(\"scaledFeatures\")\n",
    "  .setWithStd(true)\n",
    "  .setWithMean(true)\n",
    "\n",
    "// Normalize each feature to have unit standard deviation.\n",
    "val scaledData = scaler\n",
    "  .fit(result_df)\n",
    "  .transform(result_df)\n",
    "\n",
    "scaledData.select(\"numerical_features\", \"scaledFeatures\", \"categorical_features\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|(1047,[0,12,222,8...|\n",
      "|(1047,[5,12,469,8...|\n",
      "|(1047,[0,12,159,8...|\n",
      "|(1047,[2,12,650,8...|\n",
      "|(1047,[0,12,118,8...|\n",
      "|(1047,[5,13,55,84...|\n",
      "|(1047,[6,13,109,8...|\n",
      "|(1047,[11,12,96,8...|\n",
      "|(1047,[6,12,56,84...|\n",
      "|(1047,[4,13,53,84...|\n",
      "|(1047,[0,13,553,8...|\n",
      "|(1047,[4,12,14,85...|\n",
      "|(1047,[6,12,17,84...|\n",
      "|(1047,[11,13,33,8...|\n",
      "|(1047,[10,13,112,...|\n",
      "|(1047,[3,12,185,8...|\n",
      "|(1047,[4,12,16,84...|\n",
      "|(1047,[11,13,354,...|\n",
      "|(1047,[3,12,69,85...|\n",
      "|(1047,[4,12,72,84...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "vectorAssembler3: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_47adbe45661f\n",
       "pipelineVectorAssembler3: org.apache.spark.ml.Pipeline = pipeline_31820cb1f2ba\n",
       "result_df: org.apache.spark.sql.DataFrame = [loan_amnt: int, funded_amnt: int ... 120 more fields]\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val vectorAssembler3 = new VectorAssembler()\n",
    "  .setInputCols(Array(\"categorical_features\", \"scaledFeatures\"))\n",
    "  .setOutputCol(\"features\")\n",
    "\n",
    "val pipelineVectorAssembler3 = new Pipeline()\n",
    "  .setStages(Array(vectorAssembler3))\n",
    "\n",
    "val result_df = pipelineVectorAssembler3\n",
    "  .fit(scaledData)\n",
    "  .transform(scaledData)\n",
    "\n",
    "result_df.select(\"features\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labelIndexer: org.apache.spark.ml.feature.StringIndexer = strIdx_c23e880ab2d8\n",
       "df3: org.apache.spark.sql.DataFrame = [loan_amnt: int, funded_amnt: int ... 121 more fields]\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// create the labels column\n",
    "val labelIndexer = new StringIndexer()\n",
    "  .setInputCol(\"loan_status\")\n",
    "  .setOutputCol(\"label\")\n",
    "\n",
    "val df3 = labelIndexer\n",
    "  .fit(result_df)\n",
    "  .transform(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model: org.apache.spark.ml.classification.LogisticRegressionModel = LogisticRegressionModel: uid = logreg_b69dc4aee2e4, numClasses = 6, numFeatures = 1047\n",
       "predictions: org.apache.spark.sql.DataFrame = [loan_amnt: int, funded_amnt: int ... 124 more fields]\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// https://www.bmc.com/blogs/using-logistic-regression-scala-spark/\n",
    "\n",
    "val model = new LogisticRegression().fit(df3)\n",
    "\n",
    "val predictions = model.transform(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+----------+\n",
      "|            features|label|prediction|\n",
      "+--------------------+-----+----------+\n",
      "|(1047,[0,12,222,8...|  1.0|       1.0|\n",
      "|(1047,[5,12,469,8...|  0.0|       0.0|\n",
      "|(1047,[0,12,159,8...|  0.0|       0.0|\n",
      "|(1047,[2,12,650,8...|  2.0|       2.0|\n",
      "|(1047,[0,12,118,8...|  1.0|       1.0|\n",
      "|(1047,[5,13,55,84...|  2.0|       2.0|\n",
      "|(1047,[6,13,109,8...|  1.0|       1.0|\n",
      "|(1047,[11,12,96,8...|  1.0|       1.0|\n",
      "|(1047,[6,12,56,84...|  0.0|       0.0|\n",
      "|(1047,[4,13,53,84...|  3.0|       0.0|\n",
      "|(1047,[0,13,553,8...|  2.0|       2.0|\n",
      "|(1047,[4,12,14,85...|  0.0|       0.0|\n",
      "|(1047,[6,12,17,84...|  0.0|       0.0|\n",
      "|(1047,[11,13,33,8...|  0.0|       0.0|\n",
      "|(1047,[10,13,112,...|  0.0|       0.0|\n",
      "|(1047,[3,12,185,8...|  1.0|       1.0|\n",
      "|(1047,[4,12,16,84...|  0.0|       0.0|\n",
      "|(1047,[11,13,354,...|  0.0|       0.0|\n",
      "|(1047,[3,12,69,85...|  0.0|       0.0|\n",
      "|(1047,[4,12,72,84...|  1.0|       1.0|\n",
      "+--------------------+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions\n",
    "  .select (\"features\", \"label\", \"prediction\")\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "areaUnderROC = 0.9573017890033131\n",
      "areaUnderPR = 0.9065998118673585\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "binaryClassificationEvaluator: org.apache.spark.ml.evaluation.BinaryClassificationEvaluator = binEval_4b0a5cf5f331\n",
       "printlnMetric: (metricName: String)Unit\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//https://stackoverflow.com/questions/37566321/spark-random-forest-binary-classifier-metrics\n",
    "\n",
    "\n",
    "val binaryClassificationEvaluator = new BinaryClassificationEvaluator()\n",
    "  .setLabelCol(\"label\")\n",
    "  .setRawPredictionCol(\"prediction\")\n",
    "\n",
    "def printlnMetric(metricName: String): Unit = {\n",
    "  println(metricName + \" = \" + binaryClassificationEvaluator\n",
    "                                 .setMetricName(metricName)\n",
    "                                 .evaluate(predictions))\n",
    "}\n",
    "\n",
    "printlnMetric(\"areaUnderROC\")\n",
    "printlnMetric(\"areaUnderPR\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
