{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and cache data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://c080bc5f4c74:4040\n",
       "SparkContext available as 'sc' (version = 2.4.3, master = local[*], app id = local-1558778484533)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [id: string, member_id: string ... 143 more fields]\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read.format(\"csv\")\n",
    "  .option(\"inferSchema\", \"true\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .load(\"LCLoan.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-25 10:02:11,785 WARN  [Thread-4] util.Utils (Logging.scala:logWarning(66)) - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "res0: Long = 2260668\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cache\n",
    "df.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register temp table for use in Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from df\").show(5,false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop fields with population rate lower than 90%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "selectColumns: Array[String] = Array(id, member_id, loan_amnt, funded_amnt, funded_amnt_inv, term, int_rate, installment, grade, sub_grade, emp_title, emp_length, home_ownership, annual_inc, verification_status, issue_d, loan_status, pymnt_plan, url, desc, purpose, title, zip_code, addr_state, dti, delinq_2yrs, earliest_cr_line, inq_last_6mths, mths_since_last_delinq, mths_since_last_record, open_acc, pub_rec, revol_bal, revol_util, total_acc, initial_list_status, out_prncp, out_prncp_inv, total_pymnt, total_pymnt_inv, total_rec_prncp, total_rec_int, total_rec_late_fee, recoveries, collection_recovery_fee, last_pymnt_d, last_pymnt_amnt, next_pymnt_d, last_credit_pull_d, collections_12_mths_ex_med, mths_since_last_major_derog, policy_code, application_type, annual_inc_joint, dti_joint, v..."
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// get all columns\n",
    "val selectColumns = df.columns.toArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fileCount: Long = 2260668\n"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Whole file count\n",
    "val fileCount = df.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sparseThreshold: Double = 0.9\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Threshold\n",
    "val sparseThreshold = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loan_amnt: 2260668.0: 1.0\n",
      "funded_amnt: 2260668.0: 1.0\n",
      "funded_amnt_inv: 2260668.0: 1.0\n",
      "term: 2260668.0: 1.0\n",
      "int_rate: 2260668.0: 1.0\n",
      "installment: 2260668.0: 1.0\n",
      "grade: 2260668.0: 1.0\n",
      "sub_grade: 2260668.0: 1.0\n",
      "emp_title: 2093737.0: 0.9261585513662334\n",
      "emp_length: 2260668.0: 1.0\n",
      "home_ownership: 2260668.0: 1.0\n",
      "annual_inc: 2260664.0: 0.999998230611483\n",
      "verification_status: 2260668.0: 1.0\n",
      "issue_d: 2260668.0: 1.0\n",
      "loan_status: 2260668.0: 1.0\n",
      "pymnt_plan: 2260668.0: 1.0\n",
      "purpose: 2260667.0: 0.9999995576528707\n",
      "title: 2237345.0: 0.9896831379043717\n",
      "zip_code: 2260666.0: 0.9999991153057415\n",
      "addr_state: 2260667.0: 0.9999995576528707\n",
      "dti: 2258956.0: 0.9992427017147144\n",
      "delinq_2yrs: 2260638.0: 0.9999867295861223\n",
      "earliest_cr_line: 2260638.0: 0.9999867295861223\n",
      "inq_last_6mths: 2260637.0: 0.9999862872389931\n",
      "open_acc: 2260558.0: 0.9999513418157819\n",
      "pub_rec: 2260576.0: 0.9999593040641085\n",
      "revol_bal: 2260614.0: 0.9999761132550202\n",
      "revol_util: 2258834.0: 0.9991887353649452\n",
      "total_acc: 2260618.0: 0.9999778826435373\n",
      "initial_list_status: 2260651.0: 0.9999924800988027\n",
      "out_prncp: 2260651.0: 0.9999924800988027\n",
      "out_prncp_inv: 2260647.0: 0.9999907107102857\n",
      "total_pymnt: 2260655.0: 0.9999942494873196\n",
      "total_pymnt_inv: 2260657.0: 0.9999951341815781\n",
      "total_rec_prncp: 2260659.0: 0.9999960188758367\n",
      "total_rec_int: 2260665.0: 0.9999986729586122\n",
      "total_rec_late_fee: 2260665.0: 0.9999986729586122\n",
      "recoveries: 2260666.0: 0.9999991153057415\n",
      "collection_recovery_fee: 2260662.0: 0.9999973459172244\n",
      "last_pymnt_d: 2258235.0: 0.9989237694345211\n",
      "last_pymnt_amnt: 2260665.0: 0.9999986729586122\n",
      "last_credit_pull_d: 2260542.0: 0.9999442642617138\n",
      "collections_12_mths_ex_med: 2260480.0: 0.9999168387396999\n",
      "policy_code: 2260588.0: 0.9999646122296596\n",
      "application_type: 2260607.0: 0.9999730168251154\n",
      "acc_now_delinq: 2260477.0: 0.9999155116983122\n",
      "tot_coll_amt: 2190522.0: 0.9689711182712366\n",
      "tot_cur_bal: 2190491.0: 0.9689574055102297\n",
      "total_rev_hi_lim: 2190395.0: 0.9689149401858211\n",
      "acc_open_past_24mths: 2210641.0: 0.9778707001647301\n",
      "avg_cur_bal: 2190325.0: 0.9688839758867733\n",
      "bc_open_to_buy: 2185735.0: 0.9668536025634901\n",
      "bc_util: 2184598.0: 0.9663506538775265\n",
      "chargeoff_within_12_mths: 2260269.0: 0.999823503495427\n",
      "delinq_amnt: 2260443.0: 0.9999004718959175\n",
      "mo_sin_old_il_acct: 2121698.0: 0.9385270194473492\n",
      "mo_sin_old_rev_tl_op: 2190476.0: 0.9689507703032909\n",
      "mo_sin_rcnt_rev_tl_op: 2190452.0: 0.9689401539721888\n",
      "mo_sin_rcnt_tl: 2190430.0: 0.9689304223353451\n",
      "mort_acc: 2210663.0: 0.9778804318015737\n",
      "mths_since_recent_bc: 2187276.0: 0.967535259489673\n",
      "num_accts_ever_120_pd: 2190404.0: 0.9689189213099845\n",
      "num_actv_bc_tl: 2190401.0: 0.9689175942685967\n",
      "num_actv_rev_tl: 2190394.0: 0.9689144978386919\n",
      "num_bc_sats: 2202080.0: 0.9740837663911729\n",
      "num_bc_tl: 2190394.0: 0.9689144978386919\n",
      "num_il_tl: 2190399.0: 0.9689167095743382\n",
      "num_op_rev_tl: 2190399.0: 0.9689167095743382\n",
      "num_rev_accts: 2190393.0: 0.9689140554915626\n",
      "num_rev_tl_bal_gt_0: 2190394.0: 0.9689144978386919\n",
      "num_sats: 2202080.0: 0.9740837663911729\n",
      "num_tl_120dpd_2m: 2107012.0: 0.9320307095071014\n",
      "num_tl_30dpd: 2190392.0: 0.9689136131444334\n",
      "num_tl_90g_dpd_24m: 2190394.0: 0.9689144978386919\n",
      "num_tl_op_past_12m: 2190394.0: 0.9689144978386919\n",
      "pct_tl_nvr_dlq: 2190238.0: 0.968845491686528\n",
      "percent_bc_gt_75: 2185290.0: 0.9666567580909713\n",
      "pub_rec_bankruptcies: 2259064.0: 0.9992904752046741\n",
      "tax_liens: 2260363.0: 0.9998650841255771\n",
      "tot_hi_cred_lim: 2190489.0: 0.9689565208159712\n",
      "total_bal_ex_mort: 2210720.0: 0.9779056455879412\n",
      "total_bc_limit: 2210698.0: 0.9778959139510977\n",
      "total_il_high_credit_limit: 2190430.0: 0.9689304223353451\n",
      "hardship_flag: 2260419.0: 0.9998898555648154\n",
      "disbursement_method: 2260413.0: 0.9998872014820398\n",
      "debt_settlement_flag: 2260475.0: 0.9999146270040536\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "goodColumns: Array[String] = Array(loan_amnt, funded_amnt, funded_amnt_inv, term, int_rate, installment, grade, sub_grade, emp_title, emp_length, home_ownership, annual_inc, verification_status, issue_d, loan_status, pymnt_plan, purpose, title, zip_code, addr_state, dti, delinq_2yrs, earliest_cr_line, inq_last_6mths, open_acc, pub_rec, revol_bal, revol_util, total_acc, initial_list_status, out_prncp, out_prncp_inv, total_pymnt, total_pymnt_inv, total_rec_prncp, total_rec_int, total_rec_late_fee, recoveries, collection_recovery_fee, last_pymnt_d, last_pymnt_amnt, last_credit_pull_d, collections_12_mths_ex_med, policy_code, application_type, acc_now_delinq, tot_coll_amt, tot_cur_bal, total_rev_hi_lim, acc_open_past_24mths, avg_cur_bal, bc_open_to_buy, bc_util, chargeoff_within_12_mths, de..."
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Get the list of good columns\n",
    "var goodColumns = Array[String]()\n",
    "for (s <- selectColumns) {\n",
    "    \n",
    "    // get the counts of not null values for give column\n",
    "    val sqlText = s\"select count(1) as cnt from df where ${s} is not null\"\n",
    "    val not_null_count = spark.sql(sqlText)\n",
    "    val not_null_int = not_null_count.select(\"cnt\").collect().map(_(0)).toList(0).toString.toDouble\n",
    "    \n",
    "    // output the columnss with population rate above threshold\n",
    "    val populationRate = not_null_int / fileCount\n",
    "    if ( populationRate >= sparseThreshold) {\n",
    "        println(s\"${s}: ${not_null_int}: ${populationRate}\")\n",
    "        goodColumns = goodColumns :+ s\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res22: Int = 86\n"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Column number reduced to 86\n",
    "goodColumns.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "goodColumnsStr: String = loan_amnt,funded_amnt,funded_amnt_inv,term,int_rate,installment,grade,sub_grade,emp_title,emp_length,home_ownership,annual_inc,verification_status,issue_d,loan_status,pymnt_plan,purpose,title,zip_code,addr_state,dti,delinq_2yrs,earliest_cr_line,inq_last_6mths,open_acc,pub_rec,revol_bal,revol_util,total_acc,initial_list_status,out_prncp,out_prncp_inv,total_pymnt,total_pymnt_inv,total_rec_prncp,total_rec_int,total_rec_late_fee,recoveries,collection_recovery_fee,last_pymnt_d,last_pymnt_amnt,last_credit_pull_d,collections_12_mths_ex_med,policy_code,application_type,acc_now_delinq,tot_coll_amt,tot_cur_bal,total_rev_hi_lim,acc_open_past_24mths,avg_cur_bal,bc_open_to_buy,bc_util,chargeoff_within_12_mths,delinq_amnt,mo_sin_old_il_acct,mo_sin_old_rev_tl_op,mo_sin_rcnt_re..."
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val goodColumnsStr = goodColumns.mkString(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfGoodColumn: org.apache.spark.sql.DataFrame = [loan_amnt: int, funded_amnt: int ... 84 more fields]\n"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfGoodColumn = spark.sql(s\"select ${goodColumnsStr} from df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- loan_amnt: integer (nullable = true)\n",
      " |-- funded_amnt: integer (nullable = true)\n",
      " |-- funded_amnt_inv: double (nullable = true)\n",
      " |-- term: string (nullable = true)\n",
      " |-- int_rate: double (nullable = true)\n",
      " |-- installment: double (nullable = true)\n",
      " |-- grade: string (nullable = true)\n",
      " |-- sub_grade: string (nullable = true)\n",
      " |-- emp_title: string (nullable = true)\n",
      " |-- emp_length: string (nullable = true)\n",
      " |-- home_ownership: string (nullable = true)\n",
      " |-- annual_inc: string (nullable = true)\n",
      " |-- verification_status: string (nullable = true)\n",
      " |-- issue_d: string (nullable = true)\n",
      " |-- loan_status: string (nullable = true)\n",
      " |-- pymnt_plan: string (nullable = true)\n",
      " |-- purpose: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- zip_code: string (nullable = true)\n",
      " |-- addr_state: string (nullable = true)\n",
      " |-- dti: string (nullable = true)\n",
      " |-- delinq_2yrs: string (nullable = true)\n",
      " |-- earliest_cr_line: string (nullable = true)\n",
      " |-- inq_last_6mths: string (nullable = true)\n",
      " |-- open_acc: string (nullable = true)\n",
      " |-- pub_rec: string (nullable = true)\n",
      " |-- revol_bal: string (nullable = true)\n",
      " |-- revol_util: string (nullable = true)\n",
      " |-- total_acc: string (nullable = true)\n",
      " |-- initial_list_status: string (nullable = true)\n",
      " |-- out_prncp: string (nullable = true)\n",
      " |-- out_prncp_inv: string (nullable = true)\n",
      " |-- total_pymnt: string (nullable = true)\n",
      " |-- total_pymnt_inv: string (nullable = true)\n",
      " |-- total_rec_prncp: string (nullable = true)\n",
      " |-- total_rec_int: string (nullable = true)\n",
      " |-- total_rec_late_fee: string (nullable = true)\n",
      " |-- recoveries: string (nullable = true)\n",
      " |-- collection_recovery_fee: string (nullable = true)\n",
      " |-- last_pymnt_d: string (nullable = true)\n",
      " |-- last_pymnt_amnt: string (nullable = true)\n",
      " |-- last_credit_pull_d: string (nullable = true)\n",
      " |-- collections_12_mths_ex_med: string (nullable = true)\n",
      " |-- policy_code: string (nullable = true)\n",
      " |-- application_type: string (nullable = true)\n",
      " |-- acc_now_delinq: string (nullable = true)\n",
      " |-- tot_coll_amt: string (nullable = true)\n",
      " |-- tot_cur_bal: string (nullable = true)\n",
      " |-- total_rev_hi_lim: string (nullable = true)\n",
      " |-- acc_open_past_24mths: integer (nullable = true)\n",
      " |-- avg_cur_bal: string (nullable = true)\n",
      " |-- bc_open_to_buy: integer (nullable = true)\n",
      " |-- bc_util: string (nullable = true)\n",
      " |-- chargeoff_within_12_mths: double (nullable = true)\n",
      " |-- delinq_amnt: integer (nullable = true)\n",
      " |-- mo_sin_old_il_acct: double (nullable = true)\n",
      " |-- mo_sin_old_rev_tl_op: integer (nullable = true)\n",
      " |-- mo_sin_rcnt_rev_tl_op: integer (nullable = true)\n",
      " |-- mo_sin_rcnt_tl: integer (nullable = true)\n",
      " |-- mort_acc: integer (nullable = true)\n",
      " |-- mths_since_recent_bc: integer (nullable = true)\n",
      " |-- num_accts_ever_120_pd: integer (nullable = true)\n",
      " |-- num_actv_bc_tl: integer (nullable = true)\n",
      " |-- num_actv_rev_tl: integer (nullable = true)\n",
      " |-- num_bc_sats: integer (nullable = true)\n",
      " |-- num_bc_tl: integer (nullable = true)\n",
      " |-- num_il_tl: integer (nullable = true)\n",
      " |-- num_op_rev_tl: integer (nullable = true)\n",
      " |-- num_rev_accts: integer (nullable = true)\n",
      " |-- num_rev_tl_bal_gt_0: integer (nullable = true)\n",
      " |-- num_sats: integer (nullable = true)\n",
      " |-- num_tl_120dpd_2m: integer (nullable = true)\n",
      " |-- num_tl_30dpd: integer (nullable = true)\n",
      " |-- num_tl_90g_dpd_24m: integer (nullable = true)\n",
      " |-- num_tl_op_past_12m: integer (nullable = true)\n",
      " |-- pct_tl_nvr_dlq: double (nullable = true)\n",
      " |-- percent_bc_gt_75: double (nullable = true)\n",
      " |-- pub_rec_bankruptcies: integer (nullable = true)\n",
      " |-- tax_liens: integer (nullable = true)\n",
      " |-- tot_hi_cred_lim: integer (nullable = true)\n",
      " |-- total_bal_ex_mort: integer (nullable = true)\n",
      " |-- total_bc_limit: integer (nullable = true)\n",
      " |-- total_il_high_credit_limit: integer (nullable = true)\n",
      " |-- hardship_flag: string (nullable = true)\n",
      " |-- disbursement_method: string (nullable = true)\n",
      " |-- debt_settlement_flag: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfGoodColumn.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfGoodColumn.createOrReplaceTempView(\"dfGoodColumn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retain only 2017 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_2017: org.apache.spark.sql.DataFrame = [loan_amnt: int, funded_amnt: int ... 84 more fields]\n"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df_2017 = spark.sql(\"select * from dfGoodColumn where issue_d like '%2017'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res29: Long = 443579\n"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Reduced count to 443579\n",
    "df_2017.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|issue_d |\n",
      "+--------+\n",
      "|Sep-2017|\n",
      "|Jun-2017|\n",
      "|Nov-2017|\n",
      "|Feb-2017|\n",
      "|Dec-2017|\n",
      "|Aug-2017|\n",
      "|May-2017|\n",
      "|Jul-2017|\n",
      "|Jan-2017|\n",
      "|Oct-2017|\n",
      "|Mar-2017|\n",
      "|Apr-2017|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Verify all records from 2017\n",
    "df_2017.select(\"issue_d\").distinct.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2017.repartition(1).write.csv(\"LCLoan_2017.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Please ignore below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some random checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|debt_settlement_flag|count(1)|\n",
      "+--------------------+--------+\n",
      "|N                   |2227363 |\n",
      "|Y                   |33055   |\n",
      "|null                |193     |\n",
      "|Cash                |57      |\n",
      "+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select debt_settlement_flag,count(1) from dfGoodColumn group by debt_settlement_flag order by 2 desc\").show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------+--------+\n",
      "|loan_status                                        |count(1)|\n",
      "+---------------------------------------------------+--------+\n",
      "|Fully Paid                                         |1041952 |\n",
      "|Current                                            |919695  |\n",
      "|Charged Off                                        |261654  |\n",
      "|Late (31-120 days)                                 |21897   |\n",
      "|In Grace Period                                    |8952    |\n",
      "|Late (16-30 days)                                  |3737    |\n",
      "|Does not meet the credit policy. Status:Fully Paid |1988    |\n",
      "|Does not meet the credit policy. Status:Charged Off|761     |\n",
      "|Default                                            |31      |\n",
      "|Oct-2015                                           |1       |\n",
      "+---------------------------------------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select loan_status,count(1) from dfGoodColumn group by loan_status order by 2 desc\").show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Robert's sampling method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fractions: scala.collection.immutable.Map[String,Double] = Map(Default -> 1.0)\n"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fractions = Map(\"Default\" -> 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subsetDF_default: org.apache.spark.sql.DataFrame = [loan_amnt: int, funded_amnt: int ... 84 more fields]\n"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val subsetDF_default = dfGoodColumn.stat.sampleBy(\"loan_status\", fractions, 36L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsetDF_default.createOrReplaceTempView(\"subsetDF_default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+\n",
      "|loan_status|count(1)|\n",
      "+-----------+--------+\n",
      "|Default    |31      |\n",
      "+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select loan_status,count(1) from subsetDF_default group by loan_status order by 2 desc\").show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems we need to specify all values in column load_status, other wise they will be treated as 0\n",
    "\n",
    "more here\n",
    "\n",
    "https://spark.apache.org/docs/1.6.0/api/java/org/apache/spark/sql/DataFrameStatFunctions.html#sampleBy(java.lang.String,%20java.util.Map,%20long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking where is all defaults coming from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|issue_d |count(1)|\n",
      "+--------+--------+\n",
      "|Mar-2016|4       |\n",
      "|Apr-2018|3       |\n",
      "|Apr-2017|2       |\n",
      "|May-2017|2       |\n",
      "|Mar-2018|2       |\n",
      "|Jan-2016|2       |\n",
      "|Apr-2016|2       |\n",
      "|Jul-2018|2       |\n",
      "|Oct-2016|1       |\n",
      "|Sep-2017|1       |\n",
      "|Jun-2017|1       |\n",
      "|Sep-2016|1       |\n",
      "|Dec-2017|1       |\n",
      "|Dec-2016|1       |\n",
      "|Jan-2017|1       |\n",
      "|Jun-2018|1       |\n",
      "|Feb-2016|1       |\n",
      "|Oct-2017|1       |\n",
      "|May-2018|1       |\n",
      "|Mar-2017|1       |\n",
      "+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select issue_d,count(1) from dfGoodColumn where loan_status = 'Default' group by issue_d order by 2 desc\").show(false)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
