{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://75cd00854c39:4040\n",
       "SparkContext available as 'sc' (version = 2.4.3, master = local[*], app id = local-1558754856679)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-25 03:27:35,033 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(62)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2019-05-25 03:27:57,605 WARN  [Thread-4] util.Utils (Logging.scala:logWarning(66)) - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.\n",
      "+--------------------+-------+\n",
      "|         loan_status|  count|\n",
      "+--------------------+-------+\n",
      "|          Fully Paid|1041952|\n",
      "|             Default|     31|\n",
      "|     In Grace Period|   8952|\n",
      "|Does not meet the...|   1988|\n",
      "|         Charged Off| 261654|\n",
      "|            Oct-2015|      1|\n",
      "|  Late (31-120 days)|  21897|\n",
      "|             Current| 919695|\n",
      "|Does not meet the...|    761|\n",
      "|   Late (16-30 days)|   3737|\n",
      "+--------------------+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n",
       "import spark.implicits._\n",
       "df: org.apache.spark.sql.DataFrame = [id: string, member_id: string ... 143 more fields]\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//importing spark packages for this task\n",
    "\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import spark.implicits._\n",
    "\n",
    "\n",
    "//create a datafram by reading in the large csv file\n",
    "\n",
    "val df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"LCLoan.csv\")\n",
    "\n",
    "\n",
    "//show a summary of our target variable\n",
    "\n",
    "df.groupBy(\"loan_status\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|         loan_status|count|\n",
      "+--------------------+-----+\n",
      "|          Fully Paid|10449|\n",
      "|             Default|    1|\n",
      "|     In Grace Period|  100|\n",
      "|Does not meet the...|   19|\n",
      "|         Charged Off| 2666|\n",
      "|  Late (31-120 days)|  208|\n",
      "|             Current| 9184|\n",
      "|Does not meet the...|    4|\n",
      "|   Late (16-30 days)|   42|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "subsetDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: string, member_id: string ... 143 more fields]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//created a subset of our large dataset\n",
    "//at 1% of our data, this should result in about 20,000 rows\n",
    "\n",
    "var subsetDF = df.sample(false, 0.01)\n",
    "\n",
    "\n",
    "\n",
    "//display a summary of our target variable, \n",
    "//and ensure that none of the counts are too out-of-line with the large dataset\n",
    "\n",
    "subsetDF.groupBy(\"loan_status\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "//Write a Folder that contains a new csv file. (it will be called part-0000-...)\n",
    "\n",
    "subsetDF.coalesce(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\"SubsetLCLoan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: Array[String] = Array(id, member_id, loan_amnt, funded_amnt, funded_amnt_inv, term, int_rate, installment, grade, sub_grade, emp_title, emp_length, home_ownership, annual_inc, verification_status, issue_d, loan_status, pymnt_plan, url, desc, purpose, title, zip_code, addr_state, dti, delinq_2yrs, earliest_cr_line, inq_last_6mths, mths_since_last_delinq, mths_since_last_record, open_acc, pub_rec, revol_bal, revol_util, total_acc, initial_list_status, out_prncp, out_prncp_inv, total_pymnt, total_pymnt_inv, total_rec_prncp, total_rec_int, total_rec_late_fee, recoveries, collection_recovery_fee, last_pymnt_d, last_pymnt_amnt, next_pymnt_d, last_credit_pull_d, collections_12_mths_ex_med, mths_since_last_major_derog, policy_code, application_type, annual_inc_joint, dti_joint, verificati..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "//lastl we'll ned to rename the file to start EDAs.\n",
    "\n",
    "subsetDF.columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|  id|count|\n",
      "+----+-----+\n",
      "|null|22673|\n",
      "+----+-----+\n",
      "\n",
      "+---------+-----+\n",
      "|member_id|count|\n",
      "+---------+-----+\n",
      "|     null|22673|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "subsetDF.groupBy(\"id\").count().show()\n",
    "subsetDF.groupBy(\"member_id\").count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res5: org.apache.spark.sql.DataFrame = [id: string, member_id: string ... 143 more fields]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subsetDF.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subsetDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [member_id: string, loan_amnt: string ... 142 more fields]\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subsetDF = subsetDF.drop(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res11: Long = 22673\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subsetDF.filter(df(\"member_id\").isNull || df(\"member_id\") === \"\" || df(\"member_id\").isNaN).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res14: Seq[String] = WrappedArray(loan_amnt, funded_amnt, funded_amnt_inv, term, int_rate, installment, grade, sub_grade, emp_title, emp_length, home_ownership, annual_inc, verification_status, issue_d, loan_status, pymnt_plan, url, desc, purpose, title, zip_code, addr_state, dti, delinq_2yrs, earliest_cr_line, inq_last_6mths, mths_since_last_delinq, mths_since_last_record, open_acc, pub_rec, revol_bal, revol_util, total_acc, initial_list_status, out_prncp, out_prncp_inv, total_pymnt, total_pymnt_inv, total_rec_prncp, total_rec_int, total_rec_late_fee, recoveries, collection_recovery_fee, last_pymnt_d, last_pymnt_amnt, next_pymnt_d, last_credit_pull_d, collections_12_mths_ex_med, mths_since_last_major_derog, policy_code, application_type, annual_inc_joint, dti_joint, verification_status..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subsetDF.columns.toSeq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "//this ended up not deleting any columns after ~50 mintes  find a new method\n",
    "//for(colname <- subsetDF.columns.toSeq){\n",
    "//    if(subsetDF.filter(df(colName).isNull || df(colName) === \"\" || df(colName).isNaN).count() > 10000){\n",
    "//        subsetDF = subsetDF.drop(colName)\n",
    "//    }\n",
    "//}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res16: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [loan_amnt: string, funded_amnt: string ... 141 more fields]\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subsetDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
